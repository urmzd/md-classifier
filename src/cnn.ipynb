{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522f7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b57234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "import csv\n",
    "import pyppeteer as ptr\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from typing import Optional, TypeVar\n",
    "from typing import Callable\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dbc4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ecbddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ef13d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/urmzd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/urmzd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/urmzd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d338a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "PosTag = tuple[str, str]\n",
    "PosTagList = list[PosTag]\n",
    "StemWord = str\n",
    "StemWordList = list[StemWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d0fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_and_extension(file_path: str) -> tuple[str, str]:\n",
    "    regex = re.compile(r\"(.*)/(.*)\\.(.*)\")\n",
    "    return regex.match(file_path).group(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f3ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraper.\n",
    "async def download_html(browser: ptr.browser.Browser, page: ptr.page.Page, url: str, selector: str) -> Optional[str]:\n",
    "    await page.goto(url, waitUntil=\"load\", timeout=0)\n",
    "    content = await page.querySelector(selector)\n",
    "\n",
    "    html = ''\n",
    "    if content:\n",
    "        html = await page.evaluate('(element) => element.textContent', content)\n",
    "        \n",
    "    return html\n",
    "\n",
    "def write_to_resource_target(target_path: str, file_name: str, content: StemWordList, extension=\"txt\") -> None:\n",
    "    with open(f\"{target_path}/{file_name}.{extension}\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(content))\n",
    "\n",
    "\n",
    "async def get_training_data_from_folder(source_path: str, target_path: str, force=False) -> None:\n",
    "    browser = await ptr.launch(headless=True)\n",
    "    page = await browser.newPage()\n",
    "    \n",
    "    glob_pattern = \"/**/*\"\n",
    "    source_files = glob(source_path + glob_pattern, recursive=True)\n",
    "    target_files = glob(target_path + glob_pattern, recursive=True)\n",
    "    target_file_names = [get_name_and_extension(file_path)[0] for file_path in target_files]\n",
    "    \n",
    "    for file_path in source_files:\n",
    "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
    "        \n",
    "        if not file_name in target_file_names or force:\n",
    "            result = await get_training_data(browser, page, file_path)\n",
    "            write_to_resource_target(target_path, file_name, result)        \n",
    "                \n",
    "    await browser.close()\n",
    "\n",
    "async def get_training_data(browser: ptr.browser.Browser, page: ptr.page.Page, file_path: str) -> StemWordList:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    words = []\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        for _, row in df.iterrows():\n",
    "            print(row)\n",
    "            result = await download_html(browser, page, row[\"link\"], row[\"selector\"])\n",
    "            words.extend(clean_up_words(tokenize(result)))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a685f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaners.\n",
    "T = TypeVar(\"T\")\n",
    "R = TypeVar(\"R\")\n",
    "\n",
    "TestValueStrCallable = Callable[[T], str]\n",
    "TestValueBoolCallable = Callable[[T], bool]\n",
    "ValueTestFnCallable = Callable[T, TestValueStrCallable]\n",
    "FilterCallable = Callable[ValueTestFnCallable, bool]\n",
    "MapCallable = Callable[ValueTestFnCallable, str]\n",
    "\n",
    "def tokenize(data: str) -> PosTagList:\n",
    "    tokenized_words = nltk.word_tokenize(data)\n",
    "    mutated_words = nltk.pos_tag(tokenized_words)\n",
    "    return mutated_words\n",
    "\n",
    "def filter_words(x: T, test_value: TestValueStrCallable, *fns: FilterCallable) -> bool:\n",
    "    if fns:\n",
    "        if fns[0](x, test_value):\n",
    "            return filter_words(x, test_value, *fns[1:])\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "    \n",
    "def filter_by_punctuation(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
    "    return x[0] != x[1]\n",
    "\n",
    "def filter_by_stop_word(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
    "    return test_value(x) not in stopwords.words(\"english\")\n",
    "\n",
    "def regex_filter(regex: str):\n",
    "    def filter_by_regex(x: T, test_value: TestValueStrCallable = lambda t: t[0]):\n",
    "        rgx = re.compile(regex)\n",
    "        return rgx.match(test_value(x))\n",
    "    return filter_by_regex\n",
    "\n",
    "filter_by_alphabet = regex_filter(r\"^([a-zA-Z]|')+$\")\n",
    "filter_by_apostrophe = regex_filter(r\"^[^']*$\")\n",
    "\n",
    "def map_by_stem_words(x: PosTag, test_value: TestValueStrCallable = lambda t: t[0], ps=PorterStemmer()) -> StemWord:\n",
    "    return ps.stem(test_value(x), True)\n",
    "\n",
    "def map_words(x: T, test_value: TestValueStrCallable, *fns: MapCallable) -> StemWord:\n",
    "    if fns:\n",
    "        return map_words(fns[0](test_value(x)), test_value, *fns[1:])\n",
    "\n",
    "    return x\n",
    "            \n",
    "def clean_up_words(words: PosTagList) -> StemWordList:\n",
    "    filtered_words = list(\n",
    "        filter(\n",
    "        lambda x: filter_words(x, lambda x: x[0], filter_by_punctuation, filter_by_stop_word, filter_by_alphabet),\n",
    "        words\n",
    "        )\n",
    "    )\n",
    "\n",
    "    stem_words = list(map(lambda x: map_words(x, lambda x: x, map_by_stem_words), filtered_words))\n",
    "    \n",
    "    return list(\n",
    "        filter(\n",
    "            lambda x: filter_words(x, lambda x: x, filter_by_apostrophe), \n",
    "            stem_words)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e865ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "await (get_training_data_from_folder(\"../resources/sources\", \"../resources/targets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d04a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsers\n",
    "def get_cleaned_data_from_file(input_file_path: str) -> Optional[list[str]]:\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        return [word.strip(\"\\n\") for word in list(file.readlines())]\n",
    "\n",
    "def get_cleaned_data_from_folder(input_path: str) -> dict[str, PosTagList]:\n",
    "    input_files = glob(input_path + \"/**/*\", recursive=True)\n",
    "    \n",
    "    data = dict()\n",
    "    for file_path in input_files:\n",
    "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
    "        classification_data = get_cleaned_data_from_file(file_path)\n",
    "        data[file_name] = classification_data\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def group_by_tags(pos_tag_list: PosTagList) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "      @unused\n",
    "    \"\"\"\n",
    "    groups = dict()\n",
    "    \n",
    "    for value,tag in pos_tag_list:\n",
    "        if tag in groups:\n",
    "            groups[tag]\n",
    "            groups[tag].append(value)\n",
    "        else:\n",
    "            groups[tag] = [value]\n",
    "            \n",
    "    return groups\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2edefd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import WittenBellProbDist, FreqDist,LaplaceProbDist\n",
    "import numpy as np\n",
    "\n",
    "def unpack_dict_list(dict_list: dict[str, StemWordList]):\n",
    "    return [v for k in dict_list for v in dict_list[k]]\n",
    "\n",
    "def generate_sample(population: StemWordList, label: str, n_unique_words: int, chr_limit = 280, max_itr = 100, word_limit = 56):\n",
    "    freq_dist = FreqDist(population)\n",
    "    prob_dist = WittenBellProbDist(freq_dist, n_unique_words)\n",
    "    \n",
    "    samples = set()\n",
    "    chr_count = 0\n",
    "    \n",
    "    for _ in range(max_itr):\n",
    "        generated_v = prob_dist.generate()\n",
    "        \n",
    "        if len(generated_v) + chr_count < chr_limit:\n",
    "            samples.add(generated_v)\n",
    "            chr_count += 1\n",
    "            \n",
    "        if len(samples) >= word_limit:\n",
    "            break;\n",
    "            \n",
    "    return np.array(list([*samples, label])).reshape(-1, 1)\n",
    "\n",
    "def generate_samples(data: dict[str, StemWordList], n_samples = 1000):\n",
    "    #print(data)\n",
    "    n_unique_words = len(set(unpack_dict_list(data)))\n",
    "    \n",
    "    return np.array([np.array([generate_sample(data[k], k, n_unique_words) for _ in range(n_samples)]) for k in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a56eca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer\n",
    "from typing import Union\n",
    "\n",
    "def get_one_hot_encoder(population: dict[str, StemWordList]):\n",
    "    population_array = np.array(list(set(unpack_dict_list(population)))).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(population_array)\n",
    "    return encoder\n",
    "\n",
    "def get_label_encoder(labels: np.ndarray):\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(labels)\n",
    "    return encoder\n",
    "\n",
    "def transform_data_by_encoder(data: np.ndarray, encoder: Union[OneHotEncoder, LabelBinarizer]):\n",
    "    encoded_data = encoder.transform(data)\n",
    "    \n",
    "    if not isinstance(encoded_data, np.ndarray):\n",
    "        encoded_data = encoded_data.toarray()\n",
    "        \n",
    "    return encoded_data\n",
    "\n",
    "def split_into_x_y(samples: np.ndarray):\n",
    "    x = samples[:,:,:-1]\n",
    "    y = samples[:,:,-1]\n",
    "    y = y.reshape(y.shape[0]*y.shape[1], 1)\n",
    "    return (x,y)\n",
    "    \n",
    "def transform_x_y(x: np.ndarray, y: np.ndarray, in_coder: OneHotEncoder, out_coder: LabelBinarizer):   \n",
    "    encoded_x = np.stack([\n",
    "        transform_data_by_encoder(x[lbl_idx, smpl_idx], in_coder)\n",
    "        for lbl_idx in range(x.shape[0])\n",
    "        for smpl_idx in range(x.shape[1])\n",
    "    ], axis=0)\n",
    "    \n",
    "#    print(np.where(encoded_x[0][0] == 1))\n",
    "        \n",
    "    encoded_y = transform_data_by_encoder(y, out_coder)\n",
    "    \n",
    "    return (encoded_x, encoded_y)\n",
    "\n",
    "def get_x_y():\n",
    "    result = get_cleaned_data_from_folder(\"../resources/targets\")\n",
    "    samples = generate_samples(result)\n",
    "    x,y = split_into_x_y(samples)\n",
    "    input_encoder = get_one_hot_encoder(result)\n",
    "    output_encoder = get_label_encoder(y)\n",
    "    return transform_x_y(x, y, input_encoder, output_encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2da0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 20:31:22.636128: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-29 20:31:22.636252: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def run():\n",
    "    x, y = get_x_y()\n",
    "    x=x.reshape((*x.shape, 1))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    print(y_train)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    \n",
    "    \n",
    "    input_shape = x_train.shape[1:]\n",
    "    vocab_size = input_shape[-2]\n",
    "    print(vocab_size, input_shape)\n",
    "    \n",
    "    config = {\n",
    "        \"filter_sizes\": [1],\n",
    "        \"kernel_sizes\": [(3, vocab_size)],\n",
    "        \"strides\": [(1,1)]\n",
    "    }\n",
    "    \n",
    "    # Input 56 Words -> One Hot -> (56, V_S)\n",
    "    # InputLayer -> Conv_Layer -> Kernel Size (2, vocab_size) == (W,H) + Stride + 1 on Height\n",
    "    # \n",
    "    \n",
    "    encoder_input = keras.Input(shape=input_shape)\n",
    "    print(encoder_input.shape)\n",
    "    conv_layer_0 = layers.Conv2D(\n",
    "        config[\"filter_sizes\"][0], config[\"kernel_sizes\"][0], strides=config[\"strides\"][0],\n",
    "        activation=\"relu\"\n",
    "    )(encoder_input)\n",
    "    print(conv_layer_0.shape)\n",
    "    flatten_layer_0 = layers.Flatten()(conv_layer_0)\n",
    "    dense_layer_0 = layers.Dense(64, activation=\"relu\")(flatten_layer_0)\n",
    "    dense_layer_1 = layers.Dense(y_train.shape[-1], activation=\"sigmoid\")(dense_layer_0)\n",
    "    \n",
    "    model = keras.Model(encoder_input, dense_layer_1, name=\"encoder\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history = model.fit(x_train, y_train, batch_size=2, epochs=10, validation_split=0.2)\n",
    "\n",
    "    test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"Test loss:\", test_scores[0])\n",
    "    print(\"Test accuracy:\", test_scores[1])\n",
    "    \n",
    "    # [R, G, B]\n",
    "    \n",
    "    \n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842be10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8b403e565ea1794a437a21eaf7a4df5fa78d6ee8c0957d183a6329de63bb757"
  },
  "kernelspec": {
   "display_name": "mdnlp",
   "language": "python",
   "name": "mdnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
