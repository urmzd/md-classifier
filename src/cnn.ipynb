{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5cc7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import asyncio\n",
    "import pyppeteer as ptr\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from typing import Callable\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ecbddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ef13d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/urmzd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/urmzd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/urmzd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d338a95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PosTag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11904/2416709293.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPosTagList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPosTag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PosTag' is not defined"
     ]
    }
   ],
   "source": [
    "PosTag = tuple[str, str]\n",
    "PosTagList = list[PosTag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraper.\n",
    "async def download_html(browser: ptr.browser.Browser, url: str, selector: str) -> Optional[str]:\n",
    "    page = await browser.newPage()\n",
    "    await page.goto(url, waitUntil=\"load\")\n",
    "    content = await page.querySelector(selector)\n",
    "\n",
    "    html = ''\n",
    "    if content:\n",
    "        html = await page.evaluate('(element) => element.textContent', content)\n",
    "\n",
    "    return html\n",
    "\n",
    "def write_to_resource_target(file_name, content: PosTagList):\n",
    "    print(content)\n",
    "    with open(f\"../resources/targets/{file_name}\", \"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"value\", \"tag\"])\n",
    "        writer.writerows(content)\n",
    "\n",
    "\n",
    "async def get_training_data_from_folder(folder_path: str) -> None:\n",
    "    browser = await ptr.launch(headless=True)\n",
    "    files = glob(folder_path + '/**/*.csv', recursive=True)\n",
    "    \n",
    "    words = dict()\n",
    "    print(files)\n",
    "    \n",
    "    for file in files:\n",
    "        result = await get_training_data(browser, file)\n",
    "        write_to_resource_target(file.split(\"/\")[-1], result)\n",
    "        words[file] = result\n",
    "        \n",
    "                \n",
    "    await browser.close()\n",
    "\n",
    "async def get_training_data(browser: ptr.browser.Browser, file_path: str) -> PosTagList:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    words = []\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        for _, row in df.iterrows():\n",
    "            print(row)\n",
    "            result = await download_html(browser, row[\"link\"], row[\"selector\"])\n",
    "            words.extend(clean_up_words(tokenize(result)))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaners.\n",
    "def tokenize(data: str):\n",
    "    tokenized_words = nltk.word_tokenize(data)\n",
    "    tagged_words = nltk.pos_tag(tokenized_words)\n",
    "    return tagged_words\n",
    "\n",
    "def filter_words(x: PosTag, fns: list[Callable[[PosTag], bool]], keep=True) -> bool:\n",
    "    if not keep:\n",
    "        return False\n",
    "    \n",
    "    if fns:\n",
    "        return filter_words(x, fns[1:], keep=fns[0](x))\n",
    "    \n",
    "    return True\n",
    "    \n",
    "def filter_by_duplicate(x: tuple[str, str]) -> bool:\n",
    "    return x[0] != x[1]\n",
    "\n",
    "def filter_by_stop_word(x: tuple[str, str]) -> bool:\n",
    "    return x[0] not in stopwords.words(\"english\")\n",
    "\n",
    "def filter_by_alphabet(x: tuple[str, str]) -> bool:\n",
    "    regex = re.compile(\"^([a-zA-Z]|')+$\")\n",
    "    return regex.match(x[0])\n",
    "            \n",
    "def clean_up_words(words: PosTagList) -> PosTagList:\n",
    "    return list(filter(\n",
    "        lambda x: filter_words(x, [filter_by_duplicate, filter_by_stop_word, filter_by_alphabet]),\n",
    "        words\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e865ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "await (get_training_data_from_folder(\"../resources/sources\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc07428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8b403e565ea1794a437a21eaf7a4df5fa78d6ee8c0957d183a6329de63bb757"
  },
  "kernelspec": {
   "display_name": "mdnlp",
   "language": "python",
   "name": "mdnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
