{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "interpreter": {
      "hash": "a8b403e565ea1794a437a21eaf7a4df5fa78d6ee8c0957d183a6329de63bb757"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmzd/md-nlp/blob/main/src/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEPWrGDXjCRA",
        "outputId": "16073fe5-c483-4b10-a3cc-2cbae0405159"
      },
      "source": [
        "!pip install pyppeteer\n",
        "!pip install ipython ipykernel --upgrade"
      ],
      "id": "NEPWrGDXjCRA",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyppeteer in /usr/local/lib/python3.7/dist-packages (0.2.6)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (1.26.7)\n",
            "Requirement already satisfied: websockets<10.0,>=9.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (9.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (4.8.2)\n",
            "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (8.2.2)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (1.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (4.62.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.10.0.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (7.30.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (6.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython) (3.0.23)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython) (0.1.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython) (0.18.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.5)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.3.5)\n",
            "Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.1.1)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata<5 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (4.8.2)\n",
            "Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (1.12.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (4.9.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (22.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUXMwCCEkh88",
        "outputId": "3d3709c9-30bc-4373-8eae-a9ef04d75a9e"
      },
      "source": [
        "!apt-get update \n",
        "!apt-get install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!dpkg -L chromium-chromedriver"
      ],
      "id": "LUXMwCCEkh88",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://dl.google.com/linux/chrome/deb stable InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (95.0.4638.69-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "/.\n",
            "/usr\n",
            "/usr/bin\n",
            "/usr/bin/chromedriver\n",
            "/usr/lib\n",
            "/usr/lib/chromium-browser\n",
            "/usr/share\n",
            "/usr/share/doc\n",
            "/usr/share/doc/chromium-chromedriver\n",
            "/usr/share/doc/chromium-chromedriver/copyright\n",
            "/usr/lib/chromium-browser/chromedriver\n",
            "/usr/share/doc/chromium-chromedriver/changelog.Debian.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "522f7eba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374bc431-e65b-4a7c-c9cf-9e34ea3d03d1"
      },
      "source": [
        "import random\n",
        "from numpy import random as nprd\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "nprd.seed(42)"
      ],
      "id": "522f7eba",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97b57234"
      },
      "source": [
        "import asyncio\n",
        "import re\n",
        "import csv\n",
        "import pyppeteer as ptr\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from typing import Optional, TypeVar\n",
        "from typing import Callable\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "97b57234",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dbc4943"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "9dbc4943",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ecbddb"
      },
      "source": [
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context"
      ],
      "id": "58ecbddb",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ef13d0",
        "outputId": "45909b3a-5983-4077-d329-27e5a3df8eac"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "id": "77ef13d0",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0euSJ137gsvA",
        "outputId": "515939e1-a394-4003-bf8b-8ada60f34094"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "0euSJ137gsvA",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOuOyAMmhX9X"
      },
      "source": [
        "resource_path = \"/content/drive/MyDrive/resources/\"\n",
        "\n",
        "data_path = resource_path + \"data/\"\n",
        "data_source_path = data_path + \"sources\"\n",
        "data_target_path = data_path + \"targets\""
      ],
      "id": "UOuOyAMmhX9X",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d338a95"
      },
      "source": [
        "from typing import List, Tuple\n",
        "PosTag = Tuple[str, str]\n",
        "PosTagList = List[PosTag]\n",
        "StemWord = str\n",
        "StemWordList = List[StemWord]"
      ],
      "id": "2d338a95",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FLPX8f4F5HU"
      },
      "source": [
        "global_config = {\n",
        "    \"word_limit\": 56,\n",
        "    \"n_samples\": 1000,\n",
        "    \"test_size\": 0.4\n",
        "}"
      ],
      "id": "_FLPX8f4F5HU",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79d0fcad"
      },
      "source": [
        "def get_name_and_extension(file_path: str) -> Tuple[str, str]:\n",
        "    regex = re.compile(r\"(.*)/(.*)\\.(.*)\")\n",
        "    return regex.match(file_path).group(2,3)"
      ],
      "id": "79d0fcad",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38f3ffc2"
      },
      "source": [
        "async def download_html(browser: ptr.browser.Browser, page: ptr.page.Page, url: str, selector: str) -> Optional[str]:\n",
        "    await page.goto(url, waitUntil=\"load\", timeout=0)\n",
        "    print(f\"Going to collect from {url} using selector {selector}\")\n",
        "    content = await page.querySelector(selector)\n",
        "\n",
        "    html = ''\n",
        "    if content:\n",
        "        html = await page.evaluate('(element) => element.textContent', content)\n",
        "        \n",
        "    return html\n",
        "\n",
        "def write_to_resource_target(target_path: str, file_name: str, content: StemWordList, extension=\"txt\") -> None:\n",
        "    with open(f\"{target_path}/{file_name}.{extension}\", \"w\") as file:\n",
        "        file.write(\"\\n\".join(content))\n",
        "\n",
        "\n",
        "async def get_training_data_from_folder(source_path: str, target_path: str, force=False) -> None:\n",
        "    browser = await ptr.launch(\n",
        "        executablePath=\"/usr/bin/chromium-browser\", \n",
        "        args= ['--no-sandbox']\n",
        "    )\n",
        "    page = await browser.newPage()\n",
        "    \n",
        "    glob_pattern = \"/**/*\"\n",
        "    source_files = glob(source_path + glob_pattern, recursive=True)\n",
        "    target_files = glob(target_path + glob_pattern, recursive=True)\n",
        "    target_file_names = [get_name_and_extension(file_path)[0] for file_path in target_files]\n",
        "    \n",
        "    for file_path in source_files:\n",
        "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
        "        \n",
        "        if not file_name in target_file_names or force:\n",
        "            result = await get_training_data(browser, page, file_path)\n",
        "            write_to_resource_target(target_path, file_name, result)        \n",
        "                \n",
        "    await browser.close()\n",
        "\n",
        "async def get_training_data(browser: ptr.browser.Browser, page: ptr.page.Page, file_path: str) -> StemWordList:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "    words = []\n",
        "    if isinstance(df, pd.DataFrame):\n",
        "        for _, row in df.iterrows():\n",
        "            result = await download_html(browser, page, row[\"link\"], row[\"selector\"])\n",
        "            words.extend(clean_up_words(tokenize(result)))\n",
        "    \n",
        "    return words"
      ],
      "id": "38f3ffc2",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a685f5d7"
      },
      "source": [
        "T = TypeVar(\"T\")\n",
        "R = TypeVar(\"R\")\n",
        "\n",
        "TestValueStrCallable = Callable[[T], str]\n",
        "TestValueBoolCallable = Callable[[T], bool]\n",
        "ValueTestFnCallable = Callable[[T], TestValueStrCallable]\n",
        "FilterCallable = Callable[[ValueTestFnCallable], bool]\n",
        "MapCallable = Callable[[ValueTestFnCallable], str]\n",
        "\n",
        "def tokenize(data: str) -> PosTagList:\n",
        "    tokenized_words = nltk.word_tokenize(data)\n",
        "    mutated_words = nltk.pos_tag(tokenized_words)\n",
        "    return mutated_words\n",
        "\n",
        "def filter_words(x: T, test_value: TestValueStrCallable, *fns: FilterCallable) -> bool:\n",
        "    if fns:\n",
        "        if fns[0](x, test_value):\n",
        "            return filter_words(x, test_value, *fns[1:])\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "    \n",
        "def filter_by_punctuation(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
        "    return x[0] != x[1]\n",
        "\n",
        "def filter_by_stop_word(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
        "    return test_value(x) not in stopwords.words(\"english\")\n",
        "\n",
        "def regex_filter(regex: str):\n",
        "    def filter_by_regex(x: T, test_value: TestValueStrCallable = lambda t: t[0]):\n",
        "        rgx = re.compile(regex)\n",
        "        return rgx.match(test_value(x))\n",
        "    return filter_by_regex\n",
        "\n",
        "filter_by_alphabet = regex_filter(r\"^([a-zA-Z]|')+$\")\n",
        "filter_by_apostrophe = regex_filter(r\"^[^']*$\")\n",
        "\n",
        "def map_by_stem_words(x: PosTag, test_value: TestValueStrCallable = lambda t: t[0], ps=PorterStemmer()) -> StemWord:\n",
        "    return ps.stem(test_value(x)).lower()\n",
        "\n",
        "def map_words(x: T, test_value: TestValueStrCallable, *fns: MapCallable) -> StemWord:\n",
        "    if fns:\n",
        "        return map_words(fns[0](test_value(x)), test_value, *fns[1:])\n",
        "\n",
        "    return x\n",
        "            \n",
        "def clean_up_words(words: PosTagList) -> StemWordList:\n",
        "    filtered_words = set(\n",
        "        filter(\n",
        "        lambda x: filter_words(x, lambda x: x[0], filter_by_punctuation, filter_by_stop_word, filter_by_alphabet),\n",
        "        words\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stem_words = set(map(lambda x: map_words(x, lambda x: x, map_by_stem_words), filtered_words))\n",
        "    \n",
        "    return set(\n",
        "        filter(\n",
        "            lambda x: filter_words(x, lambda x: x, filter_by_apostrophe), \n",
        "            stem_words)\n",
        "    )"
      ],
      "id": "a685f5d7",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXtjX9V4sW2Q",
        "outputId": "6094b330-5a16-40cc-8386-11f47320ba2a"
      },
      "source": [
        "!curl -sSL https://dl.google.com/linux/linux_signing_key.pub | apt-key add -\n",
        "!echo \"deb [arch=amd64] https://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list\n",
        "!apt update -y && apt install -y google-chrome-stable"
      ],
      "id": "cXtjX9V4sW2Q",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n",
            "Hit:1 https://dl.google.com/linux/chrome/deb stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "43 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "google-chrome-stable is already the newest version (96.0.4664.45-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWLq2u_3qVn-",
        "outputId": "f04563a9-535a-4abb-cd34-64b2e4e8e333"
      },
      "source": [
        "await get_training_data_from_folder(data_source_path, data_target_path, force=True)"
      ],
      "id": "WWLq2u_3qVn-",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URMZD /usr/bin/chromium-browser--disable-background-networking--disable-background-timer-throttling--disable-breakpad--disable-browser-side-navigation--disable-client-side-phishing-detection--disable-default-apps--disable-dev-shm-usage--disable-extensions--disable-features=site-per-process--disable-hang-monitor--disable-popup-blocking--disable-prompt-on-repost--disable-sync--disable-translate--metrics-recording-only--no-first-run--safebrowsing-disable-auto-update--enable-automation--password-store=basic--use-mock-keychain--headless--hide-scrollbars--mute-audioabout:blank--no-sandbox--remote-debugging-port=53531--user-data-dir=/root/.local/share/pyppeteer/.dev_profile/tmpo6yoopqw\n",
            "Going to collect from https://www.uptodate.com/contents/tetanus-the-basics?search=Patient%20education&source=search_result&selectedTitle=110~150&usage_type=default&display_rank=110 using selector #topicText\n",
            "Going to collect from  https://www.mayoclinic.org/diseases-conditions/tetanus/symptoms-causes/syc-20351625 using selector #main-content\n",
            "Going to collect from https://en.wikipedia.org/wiki/Tetanus using selector #mw-content-text\n",
            "Going to collect from https://www.nhs.uk/conditions/tetanus/ using selector #maincontent > article > div > div\n",
            "Going to collect from https://www.cdc.gov/tetanus/about/symptoms-complications.html using selector body > div.container.d-flex.flex-wrap.body-wrapper.bg-white > main > div:nth-child(3) > div > div:nth-child(3) > div:nth-child(1) > div\n",
            "Going to collect from https://www.healthline.com/health/tetanus#causes using selector #__next > div.css-fdjy12 > div:nth-child(5) > div.css-stl7tm > div > div\n",
            "Going to collect from https://www.medicalnewstoday.com/articles/163063 using selector #__next > div.css-fdjy12 > div:nth-child(5) > div > div > div > div:nth-child(3) > article\n",
            "Going to collect from https://www.hopkinsmedicine.org/health/conditions-and-diseases/tetanus using selector #skip > div:nth-child(1) > article\n",
            "Going to collect from https://www.childrenshospital.org/conditions-and-treatments/conditions/t/tetanus/symptoms-and-causes using selector body > div:nth-child(6) > div > div.main-content > section.component-body-content-refactored.row.v-gutters > div > div\n",
            "Going to collect from https://www.everydayhealth.com/tetanus/guide/ using selector body > main > article > div.article-template__body > div > div.row.article-template__row--right-rail > div.col-lg-8.article-template__main > div:nth-child(2) > div > div > div > div > section\n",
            "Going to collect from https://www.uptodate.com/contents/acute-treatment-of-migraine-in-adults using selector #topicText\n",
            "Going to collect from https://www.mayoclinic.org/diseases-conditions/migraine-headache/symptoms-causes/syc-20360201 using selector #main-content\n",
            "Going to collect from http://en.wikipedia.org/wiki/Migraines using selector #mw-content-text\n",
            "Going to collect from https://www.nhs.uk/conditions/migraine/symptoms using selector #maincontent > article > div > div\n",
            "Going to collect from https://www.healthline.com/health/migraine-symptoms#recovery-stage using selector #__next > div.css-fdjy12 > div.css-stl7tm > div > div > div:nth-child(3) > article\n",
            "Going to collect from https://www.medicalnewstoday.com/articles/148373#_noHeaderPrefixedContent using selector #__next > div.css-fdjy12 > div.css-stl7tm > div > div > div:nth-child(3) > article\n",
            "Going to collect from https://www.hopkinsmedicine.org/health/conditions-and-diseases/headache/migraine-headaches using selector #skip > div:nth-child(1) > article > div:nth-child(6)\n",
            "Going to collect from https://www.everydayhealth.com/migraine/guide/ using selector #body > main > article > div.article-template__body > div > div.row.article-template__row--right-rail > div.col-lg-8.article-template__main\n",
            "Going to collect from https://www.verywellhealth.com/symptoms-of-migraine-1718209 using selector #article_1-0\n",
            "Going to collect from https://www.uptodate.com/contents/depression-the-basics?search=Patient%20education&source=search_result&selectedTitle=15~150&usage_type=default&display_rank=15 using selector #topicText\n",
            "Going to collect from https://www.mayoclinic.org/diseases-conditions/depression/symptoms-causes/syc-20356007 using selector #main-content\n",
            "Going to collect from https://simple.wikipedia.org/wiki/Depression_(mental_illness) using selector #mw-content-text\n",
            "Going to collect from https://www.nhs.uk/mental-health/conditions/clinical-depression/symptoms using selector #maincontent\n",
            "Going to collect from https://www.healthline.com/health/depression#symptoms using selector #__next > div.css-fdjy12 > div:nth-child(5) > div > div > div > div:nth-child(3)\n",
            "Going to collect from https://www.medicalnewstoday.com/articles/321385#signs-and-symptoms using selector #__next > div.css-fdjy12 > div:nth-child(5) > div.css-stl7tm > div > div\n",
            "Going to collect from https://www.hopkinsmedicine.org/health/conditions-and-diseases/depression using selector #skip > div:nth-child(1) > article\n",
            "Going to collect from https://www.childrenshospital.org/conditions-and-treatments/conditions/d/depression using selector body > div:nth-child(6) > div > div.main-content > section.component-body-content-refactored.row.v-gutters > div > div\n",
            "Going to collect from https://www.verywellmind.com/what-is-moderate-depression-5072794 using selector #mntl-sc-page_1-0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d04a53"
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "def get_cleaned_data_from_file(input_file_path: str) -> Optional[List[str]]:\n",
        "    with open(input_file_path, \"r\") as file:\n",
        "        return [word.strip(\"\\n\") for word in list(file.readlines())]\n",
        "\n",
        "def get_cleaned_data_from_folder(input_path: str) -> Dict[str, PosTagList]:\n",
        "    glob_pattern = \"/**/*\"\n",
        "    input_files = glob(input_path + glob_pattern, recursive=True)\n",
        "    \n",
        "    data = dict()\n",
        "    for file_path in input_files:\n",
        "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
        "        classification_data = get_cleaned_data_from_file(file_path)\n",
        "        data[file_name] = classification_data\n",
        "        \n",
        "    return data"
      ],
      "id": "86d04a53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2edefd61"
      },
      "source": [
        "from nltk.probability import WittenBellProbDist, FreqDist,LaplaceProbDist\n",
        "import numpy as np\n",
        "\n",
        "def unpack_dict_list(dict_list: Dict[str, StemWordList]):\n",
        "    return [v for k in dict_list for v in dict_list[k]]\n",
        "\n",
        "def generate_sample(population: StemWordList, label: str, n_unique_words: int, word_limit = global_config[\"word_limit\"]):\n",
        "    freq_dist = FreqDist(population)\n",
        "    prob_dist = WittenBellProbDist(freq_dist, n_unique_words)\n",
        "    \n",
        "    samples = [prob_dist.generate() for _ in range(word_limit)]\n",
        "\n",
        "    return np.array(list([*samples, label])).reshape(-1, 1)\n",
        "\n",
        "def generate_samples(data: Dict[str, StemWordList], n_samples = global_config[\"n_samples\"]):\n",
        "    n_unique_words = len(set(unpack_dict_list(data)))\n",
        "    \n",
        "    return np.array([np.array([generate_sample(data[k], k, n_unique_words) for _ in range(n_samples)]) for k in data])"
      ],
      "id": "2edefd61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a56eca5f"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer\n",
        "from typing import Union\n",
        "\n",
        "def get_one_hot_encoder(population: Dict[str, StemWordList]):\n",
        "    population_array = np.array(list(set(unpack_dict_list(population)))).reshape(-1, 1)\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    encoder.fit(population_array)\n",
        "    return encoder\n",
        "\n",
        "def get_label_encoder(labels: np.ndarray):\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder.fit(labels)\n",
        "    return encoder\n",
        "\n",
        "def transform_data_by_encoder(data: np.ndarray, encoder: Union[OneHotEncoder, LabelBinarizer]):\n",
        "    encoded_data = encoder.transform(data)\n",
        "    \n",
        "    if not isinstance(encoded_data, np.ndarray):\n",
        "        encoded_data = encoded_data.toarray()\n",
        "        \n",
        "    return encoded_data\n",
        "\n",
        "def split_into_x_y(samples: np.ndarray):\n",
        "    x = samples[:,:,:-1]\n",
        "    y = samples[:,:,-1]\n",
        "    y = y.reshape(y.shape[0]*y.shape[1], 1)\n",
        "    return (x,y)\n",
        "    \n",
        "def transform_x_y(x: np.ndarray, y: np.ndarray, in_coder: OneHotEncoder, out_coder: LabelBinarizer):   \n",
        "    encoded_x = np.stack([\n",
        "        transform_data_by_encoder(x[lbl_idx, smpl_idx], in_coder)\n",
        "        for lbl_idx in range(x.shape[0])\n",
        "        for smpl_idx in range(x.shape[1])\n",
        "    ], axis=0)\n",
        "            \n",
        "    encoded_y = transform_data_by_encoder(y, out_coder)\n",
        "    \n",
        "    return (encoded_x, encoded_y)"
      ],
      "id": "a56eca5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILH3Z7OzLCx4"
      },
      "source": [
        "import pickle\n",
        "\n",
        "model_path = resource_path + \"models/\"\n",
        "input_encoder_file_name = \"input_encoder.pickle\"\n",
        "output_encoder_file_name = \"output_encoder.pickle\"\n",
        "\n",
        "def get_x_y(path = data_target_path, n_samples=2000):\n",
        "    result = get_cleaned_data_from_folder(data_target_path)\n",
        "    samples = generate_samples(result, n_samples)\n",
        "    x,y = split_into_x_y(samples)\n",
        "\n",
        "    encoder_paths = glob(model_path + \"*.pickle\", recursive=True)\n",
        "    encoder_file_names = [get_name_and_extension(file_path)[0] for file_path in encoder_paths]\n",
        "\n",
        "    if input_encoder_file_name in encoder_file_names:\n",
        "      input_encoder = pickle.load(open(encoder_paths[encoder_file_names.index(input_encoder_file_name)], \"rb\"))\n",
        "    else:\n",
        "      input_encoder = get_one_hot_encoder(result)\n",
        "\n",
        "    if output_encoder_file_name in encoder_file_names:\n",
        "      output_encoder = pickle.load(open(encoder_paths[encoder_file_names.index(output_encoder_file_name)], \"rb\"))\n",
        "    else:\n",
        "      output_encoder = get_label_encoder(y)\n",
        "\n",
        "    pickle.dump(input_encoder, open(model_path + input_encoder_file_name, \"wb\"))\n",
        "    pickle.dump(output_encoder, open(model_path + output_encoder_file_name, \"wb\"))\n",
        "\n",
        "    return transform_x_y(x, y, input_encoder, output_encoder)"
      ],
      "id": "ILH3Z7OzLCx4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVI-l8QOnbMF"
      },
      "source": [
        "def get_data(n_samples=global_config[\"n_samples\"], test_size=global_config[\"test_size\"], random_state=42):\n",
        "    x, y = get_x_y(n_samples=n_samples)\n",
        "    x=x.reshape((*x.shape, 1))\n",
        "    return train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "x_train, x_test, y_train, y_test = get_data()"
      ],
      "id": "MVI-l8QOnbMF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbHPXQq5eUlK"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers, activations\n",
        "\n",
        "history_path = model_path + \"history.npy\"\n",
        "def save_history(history: np.ndarray):\n",
        "  np.save(history_path, history, allow_pickle=True)\n",
        "\n",
        "def load_history() -> Dict:\n",
        "  return np.load(history_path, allow_pickle=True).item()\n",
        "\n",
        "def run():\n",
        "    input_shape = x_train.shape[1:]\n",
        "    vocab_size = input_shape[-2]\n",
        "\n",
        "    config = {}\n",
        "\n",
        "    config[\"conv_layer_0\"] = {\n",
        "        \"filters\": 5 * vocab_size / 6,\n",
        "        \"kernel_size\": (3, vocab_size),\n",
        "        \"strides\": (1,1),\n",
        "        \"activation\": \"relu\",\n",
        "    }\n",
        "\n",
        "    config[\"pool_layer_0\"] = {\n",
        "        \"pool_size\": (3,3),\n",
        "        \"padding\": \"same\"\n",
        "    }\n",
        "\n",
        "    config[\"conv_layer_1\"] = {\n",
        "        \"filters\": 7 * config[\"conv_layer_0\"][\"filters\"] / 16,\n",
        "        \"kernel_size\": (2, 1),\n",
        "        \"strides\": (1,1),\n",
        "        \"activation\": \"relu\"\n",
        "    }\n",
        "\n",
        "    config[\"pool_layer_1\"] = {\n",
        "        \"pool_size\": (3,3),\n",
        "        \"padding\": \"same\",\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_5\"] = {\n",
        "        \"units\": 16,\n",
        "        \"activation\": \"relu\",\n",
        "        \"bias_regularizer\": regularizers.l1_l2(0.6, 0.17)\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_6\"] = {\n",
        "        \"units\": 8,\n",
        "        \"activation\": \"relu\",\n",
        "        \"bias_regularizer\": regularizers.l1_l2(0.10, 0.18)\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_7\"] = {\n",
        "        \"units\": 4,\n",
        "        \"activation\": \"relu\",\n",
        "        # \"kernel_regularizer\": regularizers.l1_l2(0.1, 0.2),\n",
        "        # \"bias_regularizer\": regularizers.l1_l2(0.1, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dropout_layer_0\"] = {\n",
        "        \"rate\": 0.5,\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_8\"]= {\n",
        "        \"units\": 8,\n",
        "        \"activation\": \"relu\",\n",
        "        # \"kernel_regularizer\": regularizers.l1_l2(0.1, 0.2),\n",
        "        # \"bias_regularizer\": regularizers.l1_l2(0.1, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dropout_layer_1\"]= {\n",
        "        \"rate\": 0.33\n",
        "    }\n",
        "\n",
        "    config[\"flatten_layer_0\"] = {\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_final\"] = {\n",
        "        \"units\": (y_train.shape[-1]),\n",
        "        \"activation\": \"softmax\"\n",
        "    }\n",
        "\n",
        "    config[\"model_compile\"] = {\n",
        "        \"optimizer\": tf.keras.optimizers.Adam(0.001),\n",
        "        \"loss\": keras.losses.binary_crossentropy,\n",
        "        \"metrics\": [\"accuracy\", keras.metrics.Recall()]\n",
        "    }\n",
        "\n",
        "    config[\"model_fit\"] = {\n",
        "         \"x\": x_train,\n",
        "         \"y\": y_train,\n",
        "         \"batch_size\": 2,\n",
        "         \"epochs\": 4,\n",
        "         \"validation_split\" : global_config[\"test_size\"]\n",
        "    }\n",
        "\n",
        "    encoder_input = keras.Input(shape=input_shape)\n",
        "    prev_layer = encoder_input\n",
        "    encoder_output=None\n",
        "\n",
        "    layer_types = {\n",
        "        \"dense\": layers.Dense,\n",
        "        \"flatten\": layers.Flatten,\n",
        "        \"dropout\": layers.Dropout,\n",
        "        \"conv\": layers.Conv2D,\n",
        "        \"pool\": layers.MaxPool2D,\n",
        "        \"comp\": None,\n",
        "    }\n",
        "    \n",
        "    for k,v in config.items():\n",
        "      type_of_layer, *_ = k.split(\"_\")\n",
        "      if type_of_layer != \"model\":\n",
        "        encoder_output = layer_types[type_of_layer](**v)(prev_layer)\n",
        "        prev_layer = encoder_output\n",
        "    \n",
        "    config[\"model\"] = {\n",
        "        \"inputs\": encoder_input,\n",
        "        \"outputs\": encoder_output,\n",
        "        \"name\": \"encoder\"\n",
        "    }\n",
        "\n",
        "    model = keras.Model(**config[\"model\"])\n",
        "    model.compile(**config[\"model_compile\"])\n",
        "\n",
        "    print(model.summary())\n",
        "    \n",
        "    history = model.fit(**config[\"model_fit\"])\n",
        "    save_history(history.history)\n",
        "\n",
        "    test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "    return model    "
      ],
      "id": "zbHPXQq5eUlK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Pj5tgVMeBh"
      },
      "source": [
        "model = run()"
      ],
      "id": "K-Pj5tgVMeBh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e2da0bd"
      },
      "source": [
        "def clean_up_user_input(sentence: str, word_limit=56):\n",
        "  tokenized_words = tokenize(sentence)\n",
        "  \n",
        "  word = clean_up_words(tokenized_words)\n",
        "  words_to_test = np.array(word).reshape(-1, 1)\n",
        "\n",
        "  if not words_to_test.size:\n",
        "    return np.array([])\n",
        "\n",
        "  input_encoder = pickle.load(open(model_path + input_encoder_file_name, \"rb\"))\n",
        "\n",
        "  encoded_data = transform_data_by_encoder(words_to_test, input_encoder) \n",
        "  difference = word_limit - encoded_data.shape[0]\n",
        "  zero_array = np.zeros(encoded_data.shape[1])\n",
        "\n",
        "  for _ in range(difference):\n",
        "    encoded_data = np.append(encoded_data, [zero_array], axis=0)\n",
        "\n",
        "  return encoded_data"
      ],
      "id": "6e2da0bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_93rstMHJ_s"
      },
      "source": [
        "import os\n",
        "\n",
        "cnn_folder = \"cnn\"\n",
        "cnn_path = model_path + cnn_folder\n",
        "\n",
        "def save_cnn(override:bool=False, model=model):\n",
        "  if not os.path.exists(cnn_path) or override:\n",
        "    model.save(cnn_path)\n",
        "\n",
        "save_cnn()"
      ],
      "id": "y_93rstMHJ_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmurpME7bffe"
      },
      "source": [
        "from keras import models\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_cnn():\n",
        "  history = load_history()\n",
        "  plt.plot(history['accuracy'])\n",
        "  plt.plot(history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "plot_cnn()"
      ],
      "id": "BmurpME7bffe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtRpg06IYRP3"
      },
      "source": [
        "def predict(sentence: str):\n",
        "  word = clean_up_user_input(sentence)\n",
        "\n",
        "  if word.size:\n",
        "    word = word.reshape(*word.shape, 1)\n",
        "    words = np.array([word])\n",
        "    output_encoder: LabelBinarizer = pickle.load(open(model_path + output_encoder_file_name, \"rb\"))\n",
        "    prediction = model.predict(words)\n",
        "    prediction_binary = np.zeros_like(prediction)\n",
        "    prediction_binary[:,prediction.argmax(1)] = 1\n",
        "    print(prediction)\n",
        "    ailment:str = output_encoder.inverse_transform(prediction)[0]\n",
        "    return ailment.upper()\n",
        "\n",
        "print(predict(\"I have a headache. I feel senstivity to light. My head is hurting. It's twirling. I feel an aura\"))\n",
        "print(predict(\"I feel suicidal. Mental health kill hurt someone else medicine need help neuropath\"))"
      ],
      "id": "KtRpg06IYRP3",
      "execution_count": null,
      "outputs": []
    }
  ]
}