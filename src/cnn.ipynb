{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "interpreter": {
      "hash": "a8b403e565ea1794a437a21eaf7a4df5fa78d6ee8c0957d183a6329de63bb757"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "522f7eba"
      },
      "source": [
        "import random\n",
        "from numpy import random as nprd\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "nprd.seed(42)"
      ],
      "id": "522f7eba",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "yP-GjFUUh318",
        "outputId": "79ace94b-be1f-47db-9d09-776f01c9116f"
      },
      "source": [
        "!pip install pyppeteer"
      ],
      "id": "yP-GjFUUh318",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-0.2.6-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 772 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (4.8.2)\n",
            "Collecting websockets<10.0,>=9.1\n",
            "  Downloading websockets-9.1-cp37-cp37m-manylinux2010_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 40.7 MB/s \n",
            "\u001b[?25hCollecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Collecting urllib3<2.0.0,>=1.25.8\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 75.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (1.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (4.62.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.10.0.2)\n",
            "Installing collected packages: websockets, urllib3, pyee, pyppeteer\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pyee-8.2.2 pyppeteer-0.2.6 urllib3-1.26.7 websockets-9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97b57234"
      },
      "source": [
        "import asyncio\n",
        "import re\n",
        "import csv\n",
        "import pyppeteer as ptr\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from typing import Optional, TypeVar\n",
        "from typing import Callable\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "97b57234",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dbc4943"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "9dbc4943",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ecbddb"
      },
      "source": [
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context"
      ],
      "id": "58ecbddb",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ef13d0",
        "outputId": "bbc67e1a-34b1-4446-d220-73fa1edcb967"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "id": "77ef13d0",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0euSJ137gsvA",
        "outputId": "5945ef7f-ef10-46a4-c3a8-e7d0a0d96683"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "0euSJ137gsvA",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOuOyAMmhX9X"
      },
      "source": [
        "resource_path = \"/content/drive/MyDrive/resources/\"\n",
        "\n",
        "data_path = resource_path + \"data/\"\n",
        "data_source_path = data_path + \"sources\"\n",
        "data_target_path = data_path + \"targets\""
      ],
      "id": "UOuOyAMmhX9X",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d338a95"
      },
      "source": [
        "from typing import List, Tuple\n",
        "PosTag = Tuple[str, str]\n",
        "PosTagList = List[PosTag]\n",
        "StemWord = str\n",
        "StemWordList = List[StemWord]"
      ],
      "id": "2d338a95",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FLPX8f4F5HU"
      },
      "source": [
        "global_config = {\n",
        "    \"word_limit\": 56,\n",
        "    \"n_samples\": 1000,\n",
        "    \"test_size\": 0.4\n",
        "}"
      ],
      "id": "_FLPX8f4F5HU",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79d0fcad"
      },
      "source": [
        "def get_name_and_extension(file_path: str) -> Tuple[str, str]:\n",
        "    regex = re.compile(r\"(.*)/(.*)\\.(.*)\")\n",
        "    return regex.match(file_path).group(2,3)"
      ],
      "id": "79d0fcad",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEPWrGDXjCRA",
        "outputId": "5e26a4c9-827f-4959-af29-a43a4d79da16"
      },
      "source": [
        "!pip install nest_asyncio\n",
        "!apt install chromium-chromedriver"
      ],
      "id": "NEPWrGDXjCRA",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 94.0 MB of archives.\n",
            "After this operation, 324 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 95.0.4638.69-0ubuntu0.18.04.1 [1,135 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 95.0.4638.69-0ubuntu0.18.04.1 [83.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 95.0.4638.69-0ubuntu0.18.04.1 [4,249 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 95.0.4638.69-0ubuntu0.18.04.1 [4,986 kB]\n",
            "Fetched 94.0 MB in 5s (19.6 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_95.0.4638.69-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUXMwCCEkh88",
        "outputId": "39f27f50-3e3a-4372-9bf4-d7b554a2c6eb"
      },
      "source": [
        "!dpkg -L chromium-chromedriver"
      ],
      "id": "LUXMwCCEkh88",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/.\n",
            "/usr\n",
            "/usr/bin\n",
            "/usr/bin/chromedriver\n",
            "/usr/lib\n",
            "/usr/lib/chromium-browser\n",
            "/usr/share\n",
            "/usr/share/doc\n",
            "/usr/share/doc/chromium-chromedriver\n",
            "/usr/share/doc/chromium-chromedriver/copyright\n",
            "/usr/lib/chromium-browser/chromedriver\n",
            "/usr/share/doc/chromium-chromedriver/changelog.Debian.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38f3ffc2"
      },
      "source": [
        "## Scraper.\n",
        "async def download_html(browser: ptr.browser.Browser, page: ptr.page.Page, url: str, selector: str) -> Optional[str]:\n",
        "    await page.goto(url, waitUntil=\"load\", timeout=0)\n",
        "    content = await page.querySelector(selector)\n",
        "\n",
        "    html = ''\n",
        "    if content:\n",
        "        html = await page.evaluate('(element) => element.textContent', content)\n",
        "        \n",
        "    return html\n",
        "\n",
        "def write_to_resource_target(target_path: str, file_name: str, content: StemWordList, extension=\"txt\") -> None:\n",
        "    with open(f\"{target_path}/{file_name}.{extension}\", \"w\") as file:\n",
        "        file.write(\"\\n\".join(content))\n",
        "\n",
        "\n",
        "async def get_training_data_from_folder(source_path: str, target_path: str, force=False) -> None:\n",
        "    browser = await ptr.launch(headless=True, executable_path=\"/usr/lib/chromium-browser/chromedriver\", options={'args': ['--no-sandbox']})\n",
        "    page = await browser.newPage()\n",
        "    \n",
        "    glob_pattern = \"/**/*\"\n",
        "    source_files = glob(source_path + glob_pattern, recursive=True)\n",
        "    target_files = glob(target_path + glob_pattern, recursive=True)\n",
        "    target_file_names = [get_name_and_extension(file_path)[0] for file_path in target_files]\n",
        "    \n",
        "    for file_path in source_files:\n",
        "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
        "        \n",
        "        if not file_name in target_file_names or force:\n",
        "            result = await get_training_data(browser, page, file_path)\n",
        "            write_to_resource_target(target_path, file_name, result)        \n",
        "                \n",
        "    await browser.close()\n",
        "\n",
        "async def get_training_data(browser: ptr.browser.Browser, page: ptr.page.Page, file_path: str) -> StemWordList:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "    words = []\n",
        "    if isinstance(df, pd.DataFrame):\n",
        "        for _, row in df.iterrows():\n",
        "            result = await download_html(browser, page, row[\"link\"], row[\"selector\"])\n",
        "            words.extend(clean_up_words(tokenize(result)))\n",
        "    \n",
        "    return words"
      ],
      "id": "38f3ffc2",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a685f5d7"
      },
      "source": [
        "## Cleaners.\n",
        "T = TypeVar(\"T\")\n",
        "R = TypeVar(\"R\")\n",
        "\n",
        "TestValueStrCallable = Callable[[T], str]\n",
        "TestValueBoolCallable = Callable[[T], bool]\n",
        "ValueTestFnCallable = Callable[[T], TestValueStrCallable]\n",
        "FilterCallable = Callable[[ValueTestFnCallable], bool]\n",
        "MapCallable = Callable[[ValueTestFnCallable], str]\n",
        "\n",
        "def tokenize(data: str) -> PosTagList:\n",
        "    tokenized_words = nltk.word_tokenize(data)\n",
        "    mutated_words = nltk.pos_tag(tokenized_words)\n",
        "    return mutated_words\n",
        "\n",
        "def filter_words(x: T, test_value: TestValueStrCallable, *fns: FilterCallable) -> bool:\n",
        "    if fns:\n",
        "        if fns[0](x, test_value):\n",
        "            return filter_words(x, test_value, *fns[1:])\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "    \n",
        "def filter_by_punctuation(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
        "    return x[0] != x[1]\n",
        "\n",
        "def filter_by_stop_word(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
        "    return test_value(x) not in stopwords.words(\"english\")\n",
        "\n",
        "def regex_filter(regex: str):\n",
        "    def filter_by_regex(x: T, test_value: TestValueStrCallable = lambda t: t[0]):\n",
        "        rgx = re.compile(regex)\n",
        "        return rgx.match(test_value(x))\n",
        "    return filter_by_regex\n",
        "\n",
        "filter_by_alphabet = regex_filter(r\"^([a-zA-Z]|')+$\")\n",
        "filter_by_apostrophe = regex_filter(r\"^[^']*$\")\n",
        "\n",
        "def map_by_stem_words(x: PosTag, test_value: TestValueStrCallable = lambda t: t[0], ps=PorterStemmer()) -> StemWord:\n",
        "    return ps.stem(test_value(x)).lower()\n",
        "\n",
        "def map_words(x: T, test_value: TestValueStrCallable, *fns: MapCallable) -> StemWord:\n",
        "    if fns:\n",
        "        return map_words(fns[0](test_value(x)), test_value, *fns[1:])\n",
        "\n",
        "    return x\n",
        "            \n",
        "def clean_up_words(words: PosTagList) -> StemWordList:\n",
        "    filtered_words = list(\n",
        "        filter(\n",
        "        lambda x: filter_words(x, lambda x: x[0], filter_by_punctuation, filter_by_stop_word, filter_by_alphabet),\n",
        "        words\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stem_words = list(map(lambda x: map_words(x, lambda x: x, map_by_stem_words), filtered_words))\n",
        "    \n",
        "    return list(\n",
        "        filter(\n",
        "            lambda x: filter_words(x, lambda x: x, filter_by_apostrophe), \n",
        "            stem_words)\n",
        "    )"
      ],
      "id": "a685f5d7",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GClsisYjJLH"
      },
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "id": "4GClsisYjJLH",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e865ee0"
      },
      "source": [
        "# type: ignore\n",
        "#asyncio.run(get_training_data_from_folder(resource_source_path, resource_target_path))"
      ],
      "id": "1e865ee0",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d04a53"
      },
      "source": [
        "# Parsers\n",
        "from typing import Dict\n",
        "\n",
        "def get_cleaned_data_from_file(input_file_path: str) -> Optional[List[str]]:\n",
        "    with open(input_file_path, \"r\") as file:\n",
        "        return [word.strip(\"\\n\") for word in list(file.readlines())]\n",
        "\n",
        "def get_cleaned_data_from_folder(input_path: str) -> Dict[str, PosTagList]:\n",
        "    glob_pattern = \"/**/*\"\n",
        "    input_files = glob(input_path + glob_pattern, recursive=True)\n",
        "    \n",
        "    data = dict()\n",
        "    for file_path in input_files:\n",
        "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
        "        classification_data = get_cleaned_data_from_file(file_path)\n",
        "        data[file_name] = classification_data\n",
        "        \n",
        "    return data\n",
        "\n",
        "def group_by_tags(pos_tag_list: PosTagList) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "      @unused\n",
        "    \"\"\"\n",
        "    groups = dict()\n",
        "    \n",
        "    for value,tag in pos_tag_list:\n",
        "        if tag in groups:\n",
        "            groups[tag]\n",
        "            groups[tag].append(value)\n",
        "        else:\n",
        "            groups[tag] = [value]\n",
        "            \n",
        "    return groups"
      ],
      "id": "86d04a53",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2edefd61"
      },
      "source": [
        "from nltk.probability import WittenBellProbDist, FreqDist,LaplaceProbDist\n",
        "import numpy as np\n",
        "\n",
        "def unpack_dict_list(dict_list: Dict[str, StemWordList]):\n",
        "    return [v for k in dict_list for v in dict_list[k]]\n",
        "\n",
        "def generate_sample(population: StemWordList, label: str, n_unique_words: int, word_limit = global_config[\"word_limit\"]):\n",
        "    freq_dist = FreqDist(population)\n",
        "    prob_dist = WittenBellProbDist(freq_dist, n_unique_words)\n",
        "    \n",
        "    samples = [prob_dist.generate() for _ in range(word_limit)]\n",
        "\n",
        "    return np.array(list([*samples, label])).reshape(-1, 1)\n",
        "\n",
        "def generate_samples(data: Dict[str, StemWordList], n_samples = global_config[\"n_samples\"]):\n",
        "    n_unique_words = len(set(unpack_dict_list(data)))\n",
        "    \n",
        "    return np.array([np.array([generate_sample(data[k], k, n_unique_words) for _ in range(n_samples)]) for k in data])"
      ],
      "id": "2edefd61",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a56eca5f"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer\n",
        "from typing import Union\n",
        "\n",
        "def get_one_hot_encoder(population: Dict[str, StemWordList]):\n",
        "    population_array = np.array(list(set(unpack_dict_list(population)))).reshape(-1, 1)\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    encoder.fit(population_array)\n",
        "    return encoder\n",
        "\n",
        "def get_label_encoder(labels: np.ndarray):\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder.fit(labels)\n",
        "    return encoder\n",
        "\n",
        "def transform_data_by_encoder(data: np.ndarray, encoder: Union[OneHotEncoder, LabelBinarizer]):\n",
        "    encoded_data = encoder.transform(data)\n",
        "    \n",
        "    if not isinstance(encoded_data, np.ndarray):\n",
        "        encoded_data = encoded_data.toarray()\n",
        "        \n",
        "    return encoded_data\n",
        "\n",
        "def split_into_x_y(samples: np.ndarray):\n",
        "    x = samples[:,:,:-1]\n",
        "    y = samples[:,:,-1]\n",
        "    y = y.reshape(y.shape[0]*y.shape[1], 1)\n",
        "    return (x,y)\n",
        "    \n",
        "def transform_x_y(x: np.ndarray, y: np.ndarray, in_coder: OneHotEncoder, out_coder: LabelBinarizer):   \n",
        "    encoded_x = np.stack([\n",
        "        transform_data_by_encoder(x[lbl_idx, smpl_idx], in_coder)\n",
        "        for lbl_idx in range(x.shape[0])\n",
        "        for smpl_idx in range(x.shape[1])\n",
        "    ], axis=0)\n",
        "            \n",
        "    encoded_y = transform_data_by_encoder(y, out_coder)\n",
        "    \n",
        "    return (encoded_x, encoded_y)\n"
      ],
      "id": "a56eca5f",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILH3Z7OzLCx4"
      },
      "source": [
        "import pickle\n",
        "\n",
        "model_path = resource_path + \"models/\"\n",
        "input_encoder_file_name = \"input_encoder.pickle\"\n",
        "output_encoder_file_name = \"output_encoder.pickle\"\n",
        "\n",
        "def get_x_y(path = data_target_path, n_samples=2000):\n",
        "    result = get_cleaned_data_from_folder(data_target_path)\n",
        "    samples = generate_samples(result, n_samples)\n",
        "    x,y = split_into_x_y(samples)\n",
        "\n",
        "    encoder_paths = glob(model_path + \"*.pickle\", recursive=True)\n",
        "    encoder_file_names = [get_name_and_extension(file_path)[0] for file_path in encoder_paths]\n",
        "\n",
        "    if input_encoder_file_name in encoder_file_names:\n",
        "      input_encoder = pickle.load(open(encoder_paths[encoder_file_names.index(input_encoder_file_name)], \"rb\"))\n",
        "    else:\n",
        "      input_encoder = get_one_hot_encoder(result)\n",
        "\n",
        "    if output_encoder_file_name in encoder_file_names:\n",
        "      output_encoder = pickle.load(open(encoder_paths[encoder_file_names.index(output_encoder_file_name)], \"rb\"))\n",
        "    else:\n",
        "      output_encoder = get_label_encoder(y)\n",
        "\n",
        "    # DUMP\n",
        "    pickle.dump(input_encoder, open(model_path + input_encoder_file_name, \"wb\"))\n",
        "    pickle.dump(output_encoder, open(model_path + output_encoder_file_name, \"wb\"))\n",
        "\n",
        "    return transform_x_y(x, y, input_encoder, output_encoder)\n",
        "\n"
      ],
      "id": "ILH3Z7OzLCx4",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVI-l8QOnbMF"
      },
      "source": [
        "def get_data(n_samples=global_config[\"n_samples\"], test_size=global_config[\"test_size\"], random_state=42):\n",
        "    x, y = get_x_y(n_samples=n_samples)\n",
        "    x=x.reshape((*x.shape, 1))\n",
        "    return train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "x_train, x_test, y_train, y_test = get_data()"
      ],
      "id": "MVI-l8QOnbMF",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbHPXQq5eUlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c22822f-8dfc-4e56-e92f-d173e798c7fa"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers, activations\n",
        "\n",
        "def run():\n",
        "    input_shape = x_train.shape[1:]\n",
        "    vocab_size = input_shape[-2]\n",
        "\n",
        "    config = {}\n",
        "\n",
        "    config[\"conv_layer_0\"] = {\n",
        "        \"filters\": 5 * vocab_size / 6,\n",
        "        \"kernel_size\": (3, vocab_size),\n",
        "        \"strides\": (1,1),\n",
        "        \"activation\": \"relu\",\n",
        "    }\n",
        "\n",
        "    config[\"pool_layer_0\"] = {\n",
        "        \"pool_size\": (3,3),\n",
        "        \"padding\": \"same\"\n",
        "    }\n",
        "\n",
        "    config[\"conv_layer_1\"] = {\n",
        "        \"filters\": 7 * config[\"conv_layer_0\"][\"filters\"] / 16,\n",
        "        \"kernel_size\": (2, 1),\n",
        "        \"strides\": (1,1),\n",
        "        \"activation\": \"relu\"\n",
        "    }\n",
        "\n",
        "    config[\"pool_layer_1\"] = {\n",
        "        \"pool_size\": (3,3),\n",
        "        \"padding\": \"same\",\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_5\"] = {\n",
        "        \"units\": 16,\n",
        "        \"activation\": \"relu\",\n",
        "        \"bias_regularizer\": regularizers.l1_l2(0.6, 0.17)\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_6\"] = {\n",
        "        \"units\": 8,\n",
        "        \"activation\": \"relu\",\n",
        "        \"bias_regularizer\": regularizers.l1_l2(0.10, 0.18)\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_7\"] = {\n",
        "        \"units\": 4,\n",
        "        \"activation\": \"relu\",\n",
        "        # \"kernel_regularizer\": regularizers.l1_l2(0.1, 0.2),\n",
        "        # \"bias_regularizer\": regularizers.l1_l2(0.1, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dropout_layer_0\"] = {\n",
        "        \"rate\": 0.5,\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_8\"]= {\n",
        "        \"units\": 8,\n",
        "        \"activation\": \"relu\",\n",
        "        # \"kernel_regularizer\": regularizers.l1_l2(0.1, 0.2),\n",
        "        # \"bias_regularizer\": regularizers.l1_l2(0.1, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dropout_layer_1\"]= {\n",
        "        \"rate\": 0.33\n",
        "    }\n",
        "\n",
        "    config[\"flatten_layer_0\"] = {\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_final\"] = {\n",
        "        \"units\": (y_train.shape[-1]),\n",
        "        \"activation\": \"softmax\"\n",
        "    }\n",
        "\n",
        "    config[\"comp_model\"]= {\n",
        "        \"optimizer\": tf.keras.optimizers.Adam(0.001),\n",
        "        \"loss\": keras.losses.binary_crossentropy,\n",
        "        \"metrics\": [\"accuracy\", keras.metrics.Recall()]\n",
        "    }\n",
        "\n",
        "    encoder_input = keras.Input(shape=input_shape)\n",
        "    prev_layer = encoder_input\n",
        "    encoder_output=None\n",
        "\n",
        "    layer_types = {\n",
        "        \"dense\": layers.Dense,\n",
        "        \"flatten\": layers.Flatten,\n",
        "        \"dropout\": layers.Dropout,\n",
        "        \"conv\": layers.Conv2D,\n",
        "        \"pool\": layers.MaxPool2D,\n",
        "        \"comp\": None,\n",
        "    }\n",
        "    \n",
        "    for k,v in config.items():\n",
        "      type_of_layer, *_ = k.split(\"_\")\n",
        "      if type_of_layer != \"comp\":\n",
        "        print(v)\n",
        "        encoder_output = layer_types[type_of_layer](**v)(prev_layer)\n",
        "        prev_layer = encoder_output\n",
        "    \n",
        "    model = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "\n",
        "    print(model.summary())\n",
        "    \n",
        "    model.compile(**config[\"comp_model\"])\n",
        "\n",
        "    history = model.fit(x_train, y_train, batch_size=2, epochs=4, validation_split=global_config[\"test_size\"])\n",
        "\n",
        "    test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "    print(\"Test loss:\", test_scores[0])\n",
        "    print(\"Test accuracy:\", test_scores[1])\n",
        "\n",
        "    return model    \n",
        "    \n",
        "model = run()"
      ],
      "id": "zbHPXQq5eUlK",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'filters': 3717.5, 'kernel_size': (3, 4461), 'strides': (1, 1), 'activation': 'relu'}\n",
            "{'pool_size': (3, 3), 'padding': 'same'}\n",
            "{'filters': 1626.40625, 'kernel_size': (2, 1), 'strides': (1, 1), 'activation': 'relu'}\n",
            "{'pool_size': (3, 3), 'padding': 'same'}\n",
            "{'units': 16, 'activation': 'relu', 'bias_regularizer': <keras.regularizers.L1L2 object at 0x7f2c64d40450>}\n",
            "{'units': 8, 'activation': 'relu', 'bias_regularizer': <keras.regularizers.L1L2 object at 0x7f2c64dd4450>}\n",
            "{'units': 4, 'activation': 'relu'}\n",
            "{'rate': 0.5}\n",
            "{'units': 8, 'activation': 'relu'}\n",
            "{'rate': 0.33}\n",
            "{}\n",
            "{'units': 3, 'activation': 'softmax'}\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 56, 4461, 1)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 54, 1, 3717)       49748328  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 18, 1, 3717)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 17, 1, 1626)       12089310  \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 1, 1626)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6, 1, 16)          26032     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6, 1, 8)           136       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6, 1, 4)           36        \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 6, 1, 4)           0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6, 1, 8)           40        \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 6, 1, 8)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 48)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 3)                 147       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,864,029\n",
            "Trainable params: 61,864,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/4\n",
            "540/540 [==============================] - 86s 157ms/step - loss: 0.6309 - accuracy: 0.3472 - recall: 0.1102 - val_loss: 0.4660 - val_accuracy: 0.3347 - val_recall: 0.3347\n",
            "Epoch 2/4\n",
            "540/540 [==============================] - 71s 131ms/step - loss: 0.4695 - accuracy: 0.6185 - recall: 0.3148 - val_loss: 0.3988 - val_accuracy: 0.6458 - val_recall: 0.3347\n",
            "Epoch 3/4\n",
            "540/540 [==============================] - 70s 130ms/step - loss: 0.4009 - accuracy: 0.6556 - recall: 0.3852 - val_loss: 0.3464 - val_accuracy: 0.6736 - val_recall: 0.3347\n",
            "Epoch 4/4\n",
            "540/540 [==============================] - 69s 127ms/step - loss: 0.3587 - accuracy: 0.6648 - recall: 0.5806 - val_loss: 0.3397 - val_accuracy: 0.6611 - val_recall: 0.3444\n",
            "38/38 - 6s - loss: 0.3402 - accuracy: 0.6633 - recall: 0.3442 - 6s/epoch - 164ms/step\n",
            "Test loss: 0.3402387499809265\n",
            "Test accuracy: 0.6633333563804626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e2da0bd"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "\n",
        "def clean_up_user_input(sentence, word_limit=56):\n",
        "  tokenized_words = tokenize(sentence)\n",
        "  \n",
        "  x = clean_up_words(tokenized_words)\n",
        "  print(x)\n",
        "  print(len(x))\n",
        "  words_to_test = np.array(x).reshape(-1, 1)\n",
        "\n",
        "  if not words_to_test.size:\n",
        "    return np.array([])\n",
        "\n",
        "  input_encoder = pickle.load(open(model_path + input_encoder_file_name, \"rb\"))\n",
        "\n",
        "  encoded_data = transform_data_by_encoder(words_to_test, input_encoder) \n",
        "  difference = word_limit - encoded_data.shape[0]\n",
        "  zero_array = np.zeros(encoded_data.shape[1])\n",
        "\n",
        "  for _ in range(difference):\n",
        "    encoded_data = np.append(encoded_data, [zero_array], axis=0)\n",
        "\n",
        "  return encoded_data"
      ],
      "id": "6e2da0bd",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_93rstMHJ_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48c1357-b71f-4185-ed84-fee720cf90ec"
      },
      "source": [
        "import os\n",
        "\n",
        "model_name = \"cnn\"\n",
        "\n",
        "if not os.path.exists(model_path + model_name):\n",
        "  model.save(model_path + model_name)"
      ],
      "id": "y_93rstMHJ_s",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/resources/models/cnn/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtRpg06IYRP3",
        "outputId": "2b960d95-28ee-4347-9c3f-c1b7c5208b35"
      },
      "source": [
        "result = get_cleaned_data_from_folder(data_target_path)\n",
        "samples = generate_samples(result, 10).reshape((-1, 57, 1))\n",
        "\n",
        "for s in samples:\n",
        "  str_word = \" \".join(list(s[:-1].reshape(-1,)))\n",
        "  str_word = \"Hallmark. Disabl. Treatment. Sensitive. Prevent. Discss. Adult\"\n",
        "  print(len(str_word))\n",
        "  word = clean_up_user_input(str_word)\n",
        "\n",
        "  if word.size == 0:\n",
        "    print(\"NOT PROCESSABLE.\")\n",
        "  else:\n",
        "    word = word.reshape(*word.shape, 1)\n",
        "    words = np.array([word])\n",
        "    output_encoder: LabelBinarizer = pickle.load(open(model_path + output_encoder_file_name, \"rb\"))\n",
        "    prediction = model.predict(words)\n",
        "    prediction_binary = np.zeros_like(prediction)\n",
        "    prediction_binary[:,prediction.argmax(1)] = 1\n",
        "    print(prediction_binary)\n",
        "    print(prediction)\n",
        "    print(output_encoder.inverse_transform(prediction))"
      ],
      "id": "KtRpg06IYRP3",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n",
            "62\n",
            "['hallmark', 'disabl', 'treatment', 'sensit', 'prevent', 'discss', 'adult']\n",
            "7\n",
            "[[0. 0. 1.]]\n",
            "[[0.03039574 0.47289145 0.49671286]]\n",
            "['tetanus']\n"
          ]
        }
      ]
    }
  ]
}