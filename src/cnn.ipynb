{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "interpreter": {
      "hash": "a8b403e565ea1794a437a21eaf7a4df5fa78d6ee8c0957d183a6329de63bb757"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NEPWrGDXjCRA",
        "outputId": "10a05b7c-96bf-46e6-e3b5-431d6ea09b20"
      },
      "source": [
        "!pip install pyppeteer\n",
        "!pip uninstall websockets\n",
        "!pip install ipython ipykernel --upgrade"
      ],
      "id": "NEPWrGDXjCRA",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-0.2.6-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 83 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (4.62.3)\n",
            "Collecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (1.4.4)\n",
            "Collecting websockets<10.0,>=9.1\n",
            "  Downloading websockets-9.1-cp37-cp37m-manylinux2010_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 39.5 MB/s \n",
            "\u001b[?25hCollecting urllib3<2.0.0,>=1.25.8\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 88.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.6.0)\n",
            "Installing collected packages: websockets, urllib3, pyee, pyppeteer\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pyee-8.2.2 pyppeteer-0.2.6 urllib3-1.26.7 websockets-9.1\n",
            "Found existing installation: websockets 9.1\n",
            "Uninstalling websockets-9.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/websockets-9.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/websockets/*\n",
            "Proceed (y/n)? \u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
            "Collecting ipython\n",
            "  Downloading ipython-7.30.1-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (4.10.1)\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.6.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 80.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython) (0.1.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.23-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 76.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.5)\n",
            "Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.1.1)\n",
            "Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (1.12.3)\n",
            "Requirement already satisfied: importlib-metadata<5 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (4.8.2)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.3.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel) (3.10.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (4.9.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (22.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel) (1.15.0)\n",
            "Installing collected packages: prompt-toolkit, ipython, ipykernel\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.23 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.6.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.1 which is incompatible.\u001b[0m\n",
            "Successfully installed ipykernel-6.6.0 ipython-7.30.1 prompt-toolkit-3.0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUXMwCCEkh88",
        "outputId": "060dc103-c4ab-4920-d189-805b8998560a"
      },
      "source": [
        "!apt-get update \n",
        "!apt-get install chromium-chromedriver\n",
        "!dpkg -L chromium-chromedriver"
      ],
      "id": "LUXMwCCEkh88",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [829 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,813 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [691 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,225 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,444 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [930 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,461 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [725 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,898 kB]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.7 kB]\n",
            "Fetched 14.4 MB in 3s (4,538 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (95.0.4638.69-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "/.\n",
            "/usr\n",
            "/usr/bin\n",
            "/usr/bin/chromedriver\n",
            "/usr/lib\n",
            "/usr/lib/chromium-browser\n",
            "/usr/share\n",
            "/usr/share/doc\n",
            "/usr/share/doc/chromium-chromedriver\n",
            "/usr/share/doc/chromium-chromedriver/copyright\n",
            "/usr/lib/chromium-browser/chromedriver\n",
            "/usr/share/doc/chromium-chromedriver/changelog.Debian.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "522f7eba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1baf1bf-8c20-415a-b885-e36062fb5c0e"
      },
      "source": [
        "import random\n",
        "from numpy import random as nprd\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "nprd.seed(42)"
      ],
      "id": "522f7eba",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97b57234"
      },
      "source": [
        "import asyncio\n",
        "import re\n",
        "import csv\n",
        "import pyppeteer as ptr\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from typing import Optional, TypeVar\n",
        "from typing import Callable\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "97b57234",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dbc4943"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "9dbc4943",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ecbddb"
      },
      "source": [
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context"
      ],
      "id": "58ecbddb",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ef13d0",
        "outputId": "397b9b33-280a-4ee7-d2d0-a17fc09d569f"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "id": "77ef13d0",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0euSJ137gsvA",
        "outputId": "dc2749da-f01b-4d14-bd2b-95fa567cf2c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "0euSJ137gsvA",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOuOyAMmhX9X"
      },
      "source": [
        "resource_path = \"/content/drive/MyDrive/resources/\"\n",
        "\n",
        "data_path = resource_path + \"data/\"\n",
        "data_source_path = data_path + \"sources\"\n",
        "data_target_path = data_path + \"targets\""
      ],
      "id": "UOuOyAMmhX9X",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d338a95"
      },
      "source": [
        "from typing import List, Tuple\n",
        "PosTag = Tuple[str, str]\n",
        "PosTagList = List[PosTag]\n",
        "StemWord = str\n",
        "StemWordList = List[StemWord]"
      ],
      "id": "2d338a95",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FLPX8f4F5HU"
      },
      "source": [
        "global_config = {\n",
        "    \"word_limit\": 56,\n",
        "    \"n_samples\": 1000,\n",
        "    \"test_size\": 0.4\n",
        "}"
      ],
      "id": "_FLPX8f4F5HU",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79d0fcad"
      },
      "source": [
        "def get_name_and_extension(file_path: str) -> Tuple[str, str]:\n",
        "    regex = re.compile(r\"(.*)/(.*)\\.(.*)\")\n",
        "    return regex.match(file_path).group(2,3)"
      ],
      "id": "79d0fcad",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38f3ffc2"
      },
      "source": [
        "async def download_html(browser: ptr.browser.Browser, page: ptr.page.Page, url: str, selector: str) -> Optional[str]:\n",
        "    await page.goto(url, waitUntil=\"load\", timeout=0)\n",
        "    content = await page.querySelector(selector)\n",
        "\n",
        "    html = ''\n",
        "    if content:\n",
        "        html = await page.evaluate('(element) => element.textContent', content)\n",
        "        \n",
        "    return html\n",
        "\n",
        "def write_to_resource_target(target_path: str, file_name: str, content: StemWordList, extension=\"txt\") -> None:\n",
        "    with open(f\"{target_path}/{file_name}.{extension}\", \"w\") as file:\n",
        "        file.write(\"\\n\".join(content))\n",
        "\n",
        "\n",
        "async def get_training_data_from_folder(source_path: str, target_path: str, force=False) -> None:\n",
        "    browser = await ptr.launch(\n",
        "        headless=True,\n",
        "        executable_path=\"/usr/bin/chromium-browser\", \n",
        "        args= ['--no-sandbox','--disable-dev-shm-usage']\n",
        "    )\n",
        "    page = await browser.newPage()\n",
        "    \n",
        "    glob_pattern = \"/**/*\"\n",
        "    source_files = glob(source_path + glob_pattern, recursive=True)\n",
        "    target_files = glob(target_path + glob_pattern, recursive=True)\n",
        "    target_file_names = [get_name_and_extension(file_path)[0] for file_path in target_files]\n",
        "    \n",
        "    for file_path in source_files:\n",
        "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
        "        \n",
        "        if not file_name in target_file_names or force:\n",
        "            result = await get_training_data(browser, page, file_path)\n",
        "            write_to_resource_target(target_path, file_name, result)        \n",
        "                \n",
        "    await browser.close()\n",
        "\n",
        "async def get_training_data(browser: ptr.browser.Browser, page: ptr.page.Page, file_path: str) -> StemWordList:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "    words = []\n",
        "    if isinstance(df, pd.DataFrame):\n",
        "        for _, row in df.iterrows():\n",
        "            result = await download_html(browser, page, row[\"link\"], row[\"selector\"])\n",
        "            words.extend(clean_up_words(tokenize(result)))\n",
        "    \n",
        "    return words"
      ],
      "id": "38f3ffc2",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a685f5d7"
      },
      "source": [
        "T = TypeVar(\"T\")\n",
        "R = TypeVar(\"R\")\n",
        "\n",
        "TestValueStrCallable = Callable[[T], str]\n",
        "TestValueBoolCallable = Callable[[T], bool]\n",
        "ValueTestFnCallable = Callable[[T], TestValueStrCallable]\n",
        "FilterCallable = Callable[[ValueTestFnCallable], bool]\n",
        "MapCallable = Callable[[ValueTestFnCallable], str]\n",
        "\n",
        "def tokenize(data: str) -> PosTagList:\n",
        "    tokenized_words = nltk.word_tokenize(data)\n",
        "    mutated_words = nltk.pos_tag(tokenized_words)\n",
        "    return mutated_words\n",
        "\n",
        "def filter_words(x: T, test_value: TestValueStrCallable, *fns: FilterCallable) -> bool:\n",
        "    if fns:\n",
        "        if fns[0](x, test_value):\n",
        "            return filter_words(x, test_value, *fns[1:])\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "    \n",
        "def filter_by_punctuation(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
        "    return x[0] != x[1]\n",
        "\n",
        "def filter_by_stop_word(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
        "    return test_value(x) not in stopwords.words(\"english\")\n",
        "\n",
        "def regex_filter(regex: str):\n",
        "    def filter_by_regex(x: T, test_value: TestValueStrCallable = lambda t: t[0]):\n",
        "        rgx = re.compile(regex)\n",
        "        return rgx.match(test_value(x))\n",
        "    return filter_by_regex\n",
        "\n",
        "filter_by_alphabet = regex_filter(r\"^([a-zA-Z]|')+$\")\n",
        "filter_by_apostrophe = regex_filter(r\"^[^']*$\")\n",
        "\n",
        "def map_by_stem_words(x: PosTag, test_value: TestValueStrCallable = lambda t: t[0], ps=PorterStemmer()) -> StemWord:\n",
        "    return ps.stem(test_value(x)).lower()\n",
        "\n",
        "def map_words(x: T, test_value: TestValueStrCallable, *fns: MapCallable) -> StemWord:\n",
        "    if fns:\n",
        "        return map_words(fns[0](test_value(x)), test_value, *fns[1:])\n",
        "\n",
        "    return x\n",
        "            \n",
        "def clean_up_words(words: PosTagList) -> StemWordList:\n",
        "    filtered_words = set(\n",
        "        filter(\n",
        "        lambda x: filter_words(x, lambda x: x[0], filter_by_punctuation, filter_by_stop_word, filter_by_alphabet),\n",
        "        words\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stem_words = set(map(lambda x: map_words(x, lambda x: x, map_by_stem_words), filtered_words))\n",
        "    \n",
        "    return set(\n",
        "        filter(\n",
        "            lambda x: filter_words(x, lambda x: x, filter_by_apostrophe), \n",
        "            stem_words)\n",
        "    )"
      ],
      "id": "a685f5d7",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GClsisYjJLH"
      },
      "source": [
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()"
      ],
      "id": "4GClsisYjJLH",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e865ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "5d88d7ab-e2dd-4511-8af6-fa78502ace1d"
      },
      "source": [
        "# type: ignore\n",
        "await get_training_data_from_folder(data_source_path, data_target_path, force=True)"
      ],
      "id": "1e865ee0",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BrowserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrowserError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_599/619657231.py\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_599/3153863269.py\u001b[0m in \u001b[0;36mget_training_data_from_folder\u001b[0;34m(source_path, target_path, force)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mheadless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mexecutable_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/usr/bin/chromium-browser\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'--no-sandbox'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyppeteer/launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(options, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0moption\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mextreme\u001b[0m \u001b[0mcaution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mLauncher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyppeteer/launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mconnectionDelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslowMo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowserWSEndpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ws_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Browser listening on: {self.browserWSEndpoint}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowserWSEndpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnectionDelay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyppeteer/launcher.py\u001b[0m in \u001b[0;36mget_ws_endpoint\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBrowserError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Browser closed unexpectedly:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrowserError\u001b[0m: Browser closed unexpectedly:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d04a53"
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "def get_cleaned_data_from_file(input_file_path: str) -> Optional[List[str]]:\n",
        "    with open(input_file_path, \"r\") as file:\n",
        "        return [word.strip(\"\\n\") for word in list(file.readlines())]\n",
        "\n",
        "def get_cleaned_data_from_folder(input_path: str) -> Dict[str, PosTagList]:\n",
        "    glob_pattern = \"/**/*\"\n",
        "    input_files = glob(input_path + glob_pattern, recursive=True)\n",
        "    \n",
        "    data = dict()\n",
        "    for file_path in input_files:\n",
        "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
        "        classification_data = get_cleaned_data_from_file(file_path)\n",
        "        data[file_name] = classification_data\n",
        "        \n",
        "    return data"
      ],
      "id": "86d04a53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2edefd61"
      },
      "source": [
        "from nltk.probability import WittenBellProbDist, FreqDist,LaplaceProbDist\n",
        "import numpy as np\n",
        "\n",
        "def unpack_dict_list(dict_list: Dict[str, StemWordList]):\n",
        "    return [v for k in dict_list for v in dict_list[k]]\n",
        "\n",
        "def generate_sample(population: StemWordList, label: str, n_unique_words: int, word_limit = global_config[\"word_limit\"]):\n",
        "    freq_dist = FreqDist(population)\n",
        "    prob_dist = WittenBellProbDist(freq_dist, n_unique_words)\n",
        "    \n",
        "    samples = [prob_dist.generate() for _ in range(word_limit)]\n",
        "\n",
        "    return np.array(list([*samples, label])).reshape(-1, 1)\n",
        "\n",
        "def generate_samples(data: Dict[str, StemWordList], n_samples = global_config[\"n_samples\"]):\n",
        "    n_unique_words = len(set(unpack_dict_list(data)))\n",
        "    \n",
        "    return np.array([np.array([generate_sample(data[k], k, n_unique_words) for _ in range(n_samples)]) for k in data])"
      ],
      "id": "2edefd61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a56eca5f"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer\n",
        "from typing import Union\n",
        "\n",
        "def get_one_hot_encoder(population: Dict[str, StemWordList]):\n",
        "    population_array = np.array(list(set(unpack_dict_list(population)))).reshape(-1, 1)\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    encoder.fit(population_array)\n",
        "    return encoder\n",
        "\n",
        "def get_label_encoder(labels: np.ndarray):\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder.fit(labels)\n",
        "    return encoder\n",
        "\n",
        "def transform_data_by_encoder(data: np.ndarray, encoder: Union[OneHotEncoder, LabelBinarizer]):\n",
        "    encoded_data = encoder.transform(data)\n",
        "    \n",
        "    if not isinstance(encoded_data, np.ndarray):\n",
        "        encoded_data = encoded_data.toarray()\n",
        "        \n",
        "    return encoded_data\n",
        "\n",
        "def split_into_x_y(samples: np.ndarray):\n",
        "    x = samples[:,:,:-1]\n",
        "    y = samples[:,:,-1]\n",
        "    y = y.reshape(y.shape[0]*y.shape[1], 1)\n",
        "    return (x,y)\n",
        "    \n",
        "def transform_x_y(x: np.ndarray, y: np.ndarray, in_coder: OneHotEncoder, out_coder: LabelBinarizer):   \n",
        "    encoded_x = np.stack([\n",
        "        transform_data_by_encoder(x[lbl_idx, smpl_idx], in_coder)\n",
        "        for lbl_idx in range(x.shape[0])\n",
        "        for smpl_idx in range(x.shape[1])\n",
        "    ], axis=0)\n",
        "            \n",
        "    encoded_y = transform_data_by_encoder(y, out_coder)\n",
        "    \n",
        "    return (encoded_x, encoded_y)"
      ],
      "id": "a56eca5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILH3Z7OzLCx4"
      },
      "source": [
        "import pickle\n",
        "\n",
        "model_path = resource_path + \"models/\"\n",
        "input_encoder_file_name = \"input_encoder.pickle\"\n",
        "output_encoder_file_name = \"output_encoder.pickle\"\n",
        "\n",
        "def get_x_y(path = data_target_path, n_samples=2000):\n",
        "    result = get_cleaned_data_from_folder(data_target_path)\n",
        "    samples = generate_samples(result, n_samples)\n",
        "    x,y = split_into_x_y(samples)\n",
        "\n",
        "    encoder_paths = glob(model_path + \"*.pickle\", recursive=True)\n",
        "    encoder_file_names = [get_name_and_extension(file_path)[0] for file_path in encoder_paths]\n",
        "\n",
        "    if input_encoder_file_name in encoder_file_names:\n",
        "      input_encoder = pickle.load(open(encoder_paths[encoder_file_names.index(input_encoder_file_name)], \"rb\"))\n",
        "    else:\n",
        "      input_encoder = get_one_hot_encoder(result)\n",
        "\n",
        "    if output_encoder_file_name in encoder_file_names:\n",
        "      output_encoder = pickle.load(open(encoder_paths[encoder_file_names.index(output_encoder_file_name)], \"rb\"))\n",
        "    else:\n",
        "      output_encoder = get_label_encoder(y)\n",
        "\n",
        "    pickle.dump(input_encoder, open(model_path + input_encoder_file_name, \"wb\"))\n",
        "    pickle.dump(output_encoder, open(model_path + output_encoder_file_name, \"wb\"))\n",
        "\n",
        "    return transform_x_y(x, y, input_encoder, output_encoder)"
      ],
      "id": "ILH3Z7OzLCx4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVI-l8QOnbMF"
      },
      "source": [
        "def get_data(n_samples=global_config[\"n_samples\"], test_size=global_config[\"test_size\"], random_state=42):\n",
        "    x, y = get_x_y(n_samples=n_samples)\n",
        "    x=x.reshape((*x.shape, 1))\n",
        "    return train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "x_train, x_test, y_train, y_test = get_data()"
      ],
      "id": "MVI-l8QOnbMF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbHPXQq5eUlK"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers, activations\n",
        "\n",
        "history_path = model_path + \"history.npy\"\n",
        "def save_history(history: np.ndarray):\n",
        "  np.save(history_path, history, allow_pickle=True)\n",
        "\n",
        "def load_history() -> Dict:\n",
        "  return np.load(history_path, allow_pickle=True).item()\n",
        "\n",
        "def run():\n",
        "    input_shape = x_train.shape[1:]\n",
        "    vocab_size = input_shape[-2]\n",
        "\n",
        "    config = {}\n",
        "\n",
        "    config[\"conv_layer_0\"] = {\n",
        "        \"filters\": 5 * vocab_size / 6,\n",
        "        \"kernel_size\": (3, vocab_size),\n",
        "        \"strides\": (1,1),\n",
        "        \"activation\": \"relu\",\n",
        "    }\n",
        "\n",
        "    config[\"pool_layer_0\"] = {\n",
        "        \"pool_size\": (3,3),\n",
        "        \"padding\": \"same\"\n",
        "    }\n",
        "\n",
        "    config[\"conv_layer_1\"] = {\n",
        "        \"filters\": 7 * config[\"conv_layer_0\"][\"filters\"] / 16,\n",
        "        \"kernel_size\": (2, 1),\n",
        "        \"strides\": (1,1),\n",
        "        \"activation\": \"relu\"\n",
        "    }\n",
        "\n",
        "    config[\"pool_layer_1\"] = {\n",
        "        \"pool_size\": (3,3),\n",
        "        \"padding\": \"same\",\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_5\"] = {\n",
        "        \"units\": 16,\n",
        "        \"activation\": \"relu\",\n",
        "        \"bias_regularizer\": regularizers.l1_l2(0.6, 0.17)\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_6\"] = {\n",
        "        \"units\": 8,\n",
        "        \"activation\": \"relu\",\n",
        "        \"bias_regularizer\": regularizers.l1_l2(0.10, 0.18)\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_7\"] = {\n",
        "        \"units\": 4,\n",
        "        \"activation\": \"relu\",\n",
        "        # \"kernel_regularizer\": regularizers.l1_l2(0.1, 0.2),\n",
        "        # \"bias_regularizer\": regularizers.l1_l2(0.1, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dropout_layer_0\"] = {\n",
        "        \"rate\": 0.5,\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_8\"]= {\n",
        "        \"units\": 8,\n",
        "        \"activation\": \"relu\",\n",
        "        # \"kernel_regularizer\": regularizers.l1_l2(0.1, 0.2),\n",
        "        # \"bias_regularizer\": regularizers.l1_l2(0.1, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dropout_layer_1\"]= {\n",
        "        \"rate\": 0.33\n",
        "    }\n",
        "\n",
        "    config[\"flatten_layer_0\"] = {\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_final\"] = {\n",
        "        \"units\": (y_train.shape[-1]),\n",
        "        \"activation\": \"softmax\"\n",
        "    }\n",
        "\n",
        "    config[\"model_compile\"] = {\n",
        "        \"optimizer\": tf.keras.optimizers.Adam(0.001),\n",
        "        \"loss\": keras.losses.binary_crossentropy,\n",
        "        \"metrics\": [\"accuracy\", keras.metrics.Recall()]\n",
        "    }\n",
        "\n",
        "    config[\"model_fit\"] = {\n",
        "         \"x\": x_train,\n",
        "         \"y\": y_train,\n",
        "         \"batch_size\": 2,\n",
        "         \"epochs\": 4,\n",
        "         \"validation_split\" : global_config[\"test_size\"]\n",
        "    }\n",
        "\n",
        "    encoder_input = keras.Input(shape=input_shape)\n",
        "    prev_layer = encoder_input\n",
        "    encoder_output=None\n",
        "\n",
        "    layer_types = {\n",
        "        \"dense\": layers.Dense,\n",
        "        \"flatten\": layers.Flatten,\n",
        "        \"dropout\": layers.Dropout,\n",
        "        \"conv\": layers.Conv2D,\n",
        "        \"pool\": layers.MaxPool2D,\n",
        "        \"comp\": None,\n",
        "    }\n",
        "    \n",
        "    for k,v in config.items():\n",
        "      type_of_layer, *_ = k.split(\"_\")\n",
        "      if type_of_layer != \"model\":\n",
        "        encoder_output = layer_types[type_of_layer](**v)(prev_layer)\n",
        "        prev_layer = encoder_output\n",
        "    \n",
        "    config[\"model\"] = {\n",
        "        \"inputs\": encoder_input,\n",
        "        \"outputs\": encoder_output,\n",
        "        \"name\": \"encoder\"\n",
        "    }\n",
        "\n",
        "    model = keras.Model(**config[\"model\"])\n",
        "    model.compile(**config[\"model_compile\"])\n",
        "\n",
        "    print(model.summary())\n",
        "    \n",
        "    history = model.fit(**config[\"model_fit\"])\n",
        "    save_history(history.history)\n",
        "\n",
        "    test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "    return model    "
      ],
      "id": "zbHPXQq5eUlK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Pj5tgVMeBh"
      },
      "source": [
        "model = run()"
      ],
      "id": "K-Pj5tgVMeBh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e2da0bd"
      },
      "source": [
        "def clean_up_user_input(sentence: str, word_limit=56):\n",
        "  tokenized_words = tokenize(sentence)\n",
        "  \n",
        "  word = clean_up_words(tokenized_words)\n",
        "  words_to_test = np.array(word).reshape(-1, 1)\n",
        "\n",
        "  if not words_to_test.size:\n",
        "    return np.array([])\n",
        "\n",
        "  input_encoder = pickle.load(open(model_path + input_encoder_file_name, \"rb\"))\n",
        "\n",
        "  encoded_data = transform_data_by_encoder(words_to_test, input_encoder) \n",
        "  difference = word_limit - encoded_data.shape[0]\n",
        "  zero_array = np.zeros(encoded_data.shape[1])\n",
        "\n",
        "  for _ in range(difference):\n",
        "    encoded_data = np.append(encoded_data, [zero_array], axis=0)\n",
        "\n",
        "  return encoded_data"
      ],
      "id": "6e2da0bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_93rstMHJ_s"
      },
      "source": [
        "import os\n",
        "\n",
        "cnn_folder = \"cnn\"\n",
        "cnn_path = model_path + cnn_folder\n",
        "\n",
        "def save_cnn(override:bool=False, model=model):\n",
        "  if not os.path.exists(cnn_path) or override:\n",
        "    model.save(cnn_path)\n",
        "\n",
        "save_cnn()"
      ],
      "id": "y_93rstMHJ_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmurpME7bffe"
      },
      "source": [
        "from keras import models\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_cnn():\n",
        "  history = load_history()\n",
        "  plt.plot(history['accuracy'])\n",
        "  plt.plot(history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "plot_cnn()"
      ],
      "id": "BmurpME7bffe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtRpg06IYRP3"
      },
      "source": [
        "def predict(sentence: str):\n",
        "  word = clean_up_user_input(sentence)\n",
        "\n",
        "  if word.size:\n",
        "    word = word.reshape(*word.shape, 1)\n",
        "    words = np.array([word])\n",
        "    output_encoder: LabelBinarizer = pickle.load(open(model_path + output_encoder_file_name, \"rb\"))\n",
        "    prediction = model.predict(words)\n",
        "    prediction_binary = np.zeros_like(prediction)\n",
        "    prediction_binary[:,prediction.argmax(1)] = 1\n",
        "    print(prediction)\n",
        "    ailment:str = output_encoder.inverse_transform(prediction)[0]\n",
        "    return ailment.upper()\n",
        "\n",
        "print(predict(\"I have a headache. I feel senstivity to light. My head is hurting. It's twirling. I feel an aura\"))\n",
        "print(predict(\"I feel suicidal. Mental health kill hurt someone else medicine need help neuropath\"))"
      ],
      "id": "KtRpg06IYRP3",
      "execution_count": null,
      "outputs": []
    }
  ]
}