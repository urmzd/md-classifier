{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "interpreter": {
      "hash": "a8b403e565ea1794a437a21eaf7a4df5fa78d6ee8c0957d183a6329de63bb757"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "522f7eba"
      },
      "source": [
        "import random\n",
        "#random.seed(42)"
      ],
      "id": "522f7eba",
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP-GjFUUh318",
        "outputId": "fcb076be-e8bb-423d-ff41-5b9ba1be2c40"
      },
      "source": [
        "!pip install pyppeteer"
      ],
      "id": "yP-GjFUUh318",
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyppeteer in /usr/local/lib/python3.7/dist-packages (0.2.6)\n",
            "Requirement already satisfied: websockets<10.0,>=9.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (9.1)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (1.4.4)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (1.26.7)\n",
            "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (8.2.2)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (4.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer) (4.62.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97b57234"
      },
      "source": [
        "import asyncio\n",
        "import re\n",
        "import csv\n",
        "import pyppeteer as ptr\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from typing import Optional, TypeVar\n",
        "from typing import Callable\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "id": "97b57234",
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dbc4943"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "9dbc4943",
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ecbddb"
      },
      "source": [
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context"
      ],
      "id": "58ecbddb",
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ef13d0",
        "outputId": "6c8b562b-b39f-4d00-8f4f-8784fb97d07d"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "id": "77ef13d0",
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0euSJ137gsvA",
        "outputId": "1324559f-db2d-45d7-95ce-cfa424fa62a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "0euSJ137gsvA",
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOuOyAMmhX9X"
      },
      "source": [
        "resource_path = \"/content/drive/MyDrive/resources/\"\n",
        "\n",
        "data_path = resource_path + \"data/\"\n",
        "data_source_path = data_path + \"sources\"\n",
        "data_target_path = data_path + \"targets\""
      ],
      "id": "UOuOyAMmhX9X",
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d338a95"
      },
      "source": [
        "from typing import List, Tuple\n",
        "PosTag = Tuple[str, str]\n",
        "PosTagList = List[PosTag]\n",
        "StemWord = str\n",
        "StemWordList = List[StemWord]"
      ],
      "id": "2d338a95",
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79d0fcad"
      },
      "source": [
        "def get_name_and_extension(file_path: str) -> Tuple[str, str]:\n",
        "    regex = re.compile(r\"(.*)/(.*)\\.(.*)\")\n",
        "    return regex.match(file_path).group(2,3)"
      ],
      "id": "79d0fcad",
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEPWrGDXjCRA",
        "outputId": "333f38e1-26b2-4111-9f86-f1cd197342a4"
      },
      "source": [
        "!pip install nest_asyncio\n",
        "!apt install chromium-chromedriver"
      ],
      "id": "NEPWrGDXjCRA",
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (95.0.4638.69-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUXMwCCEkh88",
        "outputId": "ebf6e64b-ab98-470a-ba20-39419a73d31a"
      },
      "source": [
        "!dpkg -L chromium-chromedriver"
      ],
      "id": "LUXMwCCEkh88",
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/.\n",
            "/usr\n",
            "/usr/bin\n",
            "/usr/bin/chromedriver\n",
            "/usr/lib\n",
            "/usr/lib/chromium-browser\n",
            "/usr/share\n",
            "/usr/share/doc\n",
            "/usr/share/doc/chromium-chromedriver\n",
            "/usr/share/doc/chromium-chromedriver/copyright\n",
            "/usr/lib/chromium-browser/chromedriver\n",
            "/usr/share/doc/chromium-chromedriver/changelog.Debian.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38f3ffc2"
      },
      "source": [
        "## Scraper.\n",
        "async def download_html(browser: ptr.browser.Browser, page: ptr.page.Page, url: str, selector: str) -> Optional[str]:\n",
        "    await page.goto(url, waitUntil=\"load\", timeout=0)\n",
        "    content = await page.querySelector(selector)\n",
        "\n",
        "    html = ''\n",
        "    if content:\n",
        "        html = await page.evaluate('(element) => element.textContent', content)\n",
        "        \n",
        "    return html\n",
        "\n",
        "def write_to_resource_target(target_path: str, file_name: str, content: StemWordList, extension=\"txt\") -> None:\n",
        "    with open(f\"{target_path}/{file_name}.{extension}\", \"w\") as file:\n",
        "        file.write(\"\\n\".join(content))\n",
        "\n",
        "\n",
        "async def get_training_data_from_folder(source_path: str, target_path: str, force=False) -> None:\n",
        "    browser = await ptr.launch(headless=True, executable_path=\"/usr/lib/chromium-browser/chromedriver\", options={'args': ['--no-sandbox']})\n",
        "    page = await browser.newPage()\n",
        "    \n",
        "    glob_pattern = \"/**/*\"\n",
        "    source_files = glob(source_path + glob_pattern, recursive=True)\n",
        "    target_files = glob(target_path + glob_pattern, recursive=True)\n",
        "    target_file_names = [get_name_and_extension(file_path)[0] for file_path in target_files]\n",
        "    \n",
        "    for file_path in source_files:\n",
        "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
        "        \n",
        "        if not file_name in target_file_names or force:\n",
        "            result = await get_training_data(browser, page, file_path)\n",
        "            write_to_resource_target(target_path, file_name, result)        \n",
        "                \n",
        "    await browser.close()\n",
        "\n",
        "async def get_training_data(browser: ptr.browser.Browser, page: ptr.page.Page, file_path: str) -> StemWordList:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "    words = []\n",
        "    if isinstance(df, pd.DataFrame):\n",
        "        for _, row in df.iterrows():\n",
        "            result = await download_html(browser, page, row[\"link\"], row[\"selector\"])\n",
        "            words.extend(clean_up_words(tokenize(result)))\n",
        "    \n",
        "    return words"
      ],
      "id": "38f3ffc2",
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a685f5d7"
      },
      "source": [
        "## Cleaners.\n",
        "T = TypeVar(\"T\")\n",
        "R = TypeVar(\"R\")\n",
        "\n",
        "TestValueStrCallable = Callable[[T], str]\n",
        "TestValueBoolCallable = Callable[[T], bool]\n",
        "ValueTestFnCallable = Callable[[T], TestValueStrCallable]\n",
        "FilterCallable = Callable[[ValueTestFnCallable], bool]\n",
        "MapCallable = Callable[[ValueTestFnCallable], str]\n",
        "\n",
        "def tokenize(data: str) -> PosTagList:\n",
        "    tokenized_words = nltk.word_tokenize(data)\n",
        "    mutated_words = nltk.pos_tag(tokenized_words)\n",
        "    return mutated_words\n",
        "\n",
        "def filter_words(x: T, test_value: TestValueStrCallable, *fns: FilterCallable) -> bool:\n",
        "    if fns:\n",
        "        if fns[0](x, test_value):\n",
        "            return filter_words(x, test_value, *fns[1:])\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "    \n",
        "def filter_by_punctuation(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
        "    return x[0] != x[1]\n",
        "\n",
        "def filter_by_stop_word(x: T, test_value: TestValueStrCallable = lambda t: t[0]) -> bool:\n",
        "    return test_value(x) not in stopwords.words(\"english\")\n",
        "\n",
        "def regex_filter(regex: str):\n",
        "    def filter_by_regex(x: T, test_value: TestValueStrCallable = lambda t: t[0]):\n",
        "        rgx = re.compile(regex)\n",
        "        return rgx.match(test_value(x))\n",
        "    return filter_by_regex\n",
        "\n",
        "filter_by_alphabet = regex_filter(r\"^([a-zA-Z]|')+$\")\n",
        "filter_by_apostrophe = regex_filter(r\"^[^']*$\")\n",
        "\n",
        "def map_by_stem_words(x: PosTag, test_value: TestValueStrCallable = lambda t: t[0], ps=PorterStemmer()) -> StemWord:\n",
        "    return ps.stem(test_value(x)).lower()\n",
        "\n",
        "def map_words(x: T, test_value: TestValueStrCallable, *fns: MapCallable) -> StemWord:\n",
        "    if fns:\n",
        "        return map_words(fns[0](test_value(x)), test_value, *fns[1:])\n",
        "\n",
        "    return x\n",
        "            \n",
        "def clean_up_words(words: PosTagList) -> StemWordList:\n",
        "    filtered_words = list(\n",
        "        filter(\n",
        "        lambda x: filter_words(x, lambda x: x[0], filter_by_punctuation, filter_by_stop_word, filter_by_alphabet),\n",
        "        words\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stem_words = list(map(lambda x: map_words(x, lambda x: x, map_by_stem_words), filtered_words))\n",
        "    \n",
        "    return list(\n",
        "        filter(\n",
        "            lambda x: filter_words(x, lambda x: x, filter_by_apostrophe), \n",
        "            stem_words)\n",
        "    )"
      ],
      "id": "a685f5d7",
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GClsisYjJLH"
      },
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "id": "4GClsisYjJLH",
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e865ee0"
      },
      "source": [
        "# type: ignore\n",
        "#asyncio.run(get_training_data_from_folder(resource_source_path, resource_target_path))"
      ],
      "id": "1e865ee0",
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d04a53"
      },
      "source": [
        "# Parsers\n",
        "from typing import Dict\n",
        "\n",
        "def get_cleaned_data_from_file(input_file_path: str) -> Optional[List[str]]:\n",
        "    with open(input_file_path, \"r\") as file:\n",
        "        return [word.strip(\"\\n\") for word in list(file.readlines())]\n",
        "\n",
        "def get_cleaned_data_from_folder(input_path: str) -> Dict[str, PosTagList]:\n",
        "    glob_pattern = \"/**/*\"\n",
        "    input_files = glob(input_path + glob_pattern, recursive=True)\n",
        "    \n",
        "    data = dict()\n",
        "    for file_path in input_files:\n",
        "        [file_name, file_extension] = get_name_and_extension(file_path)\n",
        "        classification_data = get_cleaned_data_from_file(file_path)\n",
        "        data[file_name] = classification_data\n",
        "        \n",
        "    return data\n",
        "\n",
        "\n",
        "def group_by_tags(pos_tag_list: PosTagList) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "      @unused\n",
        "    \"\"\"\n",
        "    groups = dict()\n",
        "    \n",
        "    for value,tag in pos_tag_list:\n",
        "        if tag in groups:\n",
        "            groups[tag]\n",
        "            groups[tag].append(value)\n",
        "        else:\n",
        "            groups[tag] = [value]\n",
        "            \n",
        "    return groups"
      ],
      "id": "86d04a53",
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2edefd61"
      },
      "source": [
        "from nltk.probability import WittenBellProbDist, FreqDist,LaplaceProbDist\n",
        "import numpy as np\n",
        "\n",
        "def unpack_dict_list(dict_list: Dict[str, StemWordList]):\n",
        "    return [v for k in dict_list for v in dict_list[k]]\n",
        "\n",
        "def generate_sample(population: StemWordList, label: str, n_unique_words: int, word_limit = 56):\n",
        "    freq_dist = FreqDist(population)\n",
        "    prob_dist = WittenBellProbDist(freq_dist, n_unique_words)\n",
        "    \n",
        "    new_word_limit = word_limit #random.randint(1, word_limit)\n",
        "\n",
        "    samples = [prob_dist.generate() for _ in range(word_limit)]\n",
        "\n",
        "    return np.array(list([*samples, label])).reshape(-1, 1)\n",
        "\n",
        "def generate_samples(data: Dict[str, StemWordList], n_samples = 1000):\n",
        "    n_unique_words = len(set(unpack_dict_list(data)))\n",
        "    \n",
        "    return np.array([np.array([generate_sample(data[k], k, n_unique_words) for _ in range(n_samples)]) for k in data])"
      ],
      "id": "2edefd61",
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a56eca5f"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer\n",
        "from typing import Union\n",
        "\n",
        "def get_one_hot_encoder(population: Dict[str, StemWordList]):\n",
        "    population_array = np.array(list(set(unpack_dict_list(population)))).reshape(-1, 1)\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    encoder.fit(population_array)\n",
        "    return encoder\n",
        "\n",
        "def get_label_encoder(labels: np.ndarray):\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder.fit(labels)\n",
        "    return encoder\n",
        "\n",
        "def transform_data_by_encoder(data: np.ndarray, encoder: Union[OneHotEncoder, LabelBinarizer]):\n",
        "    encoded_data = encoder.transform(data)\n",
        "    \n",
        "    if not isinstance(encoded_data, np.ndarray):\n",
        "        encoded_data = encoded_data.toarray()\n",
        "        \n",
        "    return encoded_data\n",
        "\n",
        "def split_into_x_y(samples: np.ndarray):\n",
        "    x = samples[:,:,:-1]\n",
        "    y = samples[:,:,-1]\n",
        "    y = y.reshape(y.shape[0]*y.shape[1], 1)\n",
        "    return (x,y)\n",
        "    \n",
        "def transform_x_y(x: np.ndarray, y: np.ndarray, in_coder: OneHotEncoder, out_coder: LabelBinarizer):   \n",
        "    encoded_x = np.stack([\n",
        "        transform_data_by_encoder(x[lbl_idx, smpl_idx], in_coder)\n",
        "        for lbl_idx in range(x.shape[0])\n",
        "        for smpl_idx in range(x.shape[1])\n",
        "    ], axis=0)\n",
        "            \n",
        "    encoded_y = transform_data_by_encoder(y, out_coder)\n",
        "    \n",
        "    return (encoded_x, encoded_y)\n"
      ],
      "id": "a56eca5f",
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILH3Z7OzLCx4"
      },
      "source": [
        "import pickle\n",
        "\n",
        "model_path = resource_path + \"models/\"\n",
        "input_encoder_file_name = \"input_encoder.pickle\"\n",
        "output_encoder_file_name = \"output_encoder.pickle\"\n",
        "\n",
        "def get_x_y(path = data_target_path, n_samples=200):\n",
        "    result = get_cleaned_data_from_folder(data_target_path)\n",
        "    samples = generate_samples(result, n_samples)\n",
        "    x,y = split_into_x_y(samples)\n",
        "\n",
        "    encoder_paths = glob(model_path + \"*.pickle\", recursive=True)\n",
        "    encoder_file_names = [get_name_and_extension(file_path)[0] for file_path in encoder_paths]\n",
        "\n",
        "    if input_encoder_file_name in encoder_file_names:\n",
        "      input_encoder = pickle.load(open(encoder_paths[encoder_file_names.index(input_encoder_file_name)], \"rb\"))\n",
        "    else:\n",
        "      input_encoder = get_one_hot_encoder(result)\n",
        "\n",
        "    if output_encoder_file_name in encoder_file_names:\n",
        "      output_encoder = pickle.load(open(encoder_paths[encoder_file_names.index(output_encoder_file_name)], \"rb\"))\n",
        "    else:\n",
        "      output_encoder = get_label_encoder(y)\n",
        "\n",
        "    # DUMP\n",
        "    pickle.dump(input_encoder, open(model_path + input_encoder_file_name, \"wb\"))\n",
        "    pickle.dump(output_encoder, open(model_path + output_encoder_file_name, \"wb\"))\n",
        "\n",
        "    return transform_x_y(x, y, input_encoder, output_encoder)\n",
        "\n"
      ],
      "id": "ILH3Z7OzLCx4",
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e2da0bd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def clean_up_user_input(sentence, word_limit=56):\n",
        "  tokenized_words = tokenize(sentence)\n",
        "  word_to_test = np.array(clean_up_words(tokenized_words)).reshape(-1, 1)\n",
        "\n",
        "  input_encoder = pickle.load(open(model_path + input_encoder_file_name, \"rb\"))\n",
        "\n",
        "  encoded_data = transform_data_by_encoder(word_to_test, input_encoder)\n",
        "  difference = word_limit - encoded_data.shape[0]\n",
        "  zero_array = np.zeros(encoded_data.shape[1])\n",
        "\n",
        "  for _ in range(difference):\n",
        "    encoded_data = np.append(encoded_data, [zero_array], axis=0)\n",
        "\n",
        "  return encoded_data"
      ],
      "id": "6e2da0bd",
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVI-l8QOnbMF"
      },
      "source": [
        "def get_data(n_samples=200, test_size=0.4, random_state=random.randint(0, 100)):\n",
        "    x, y = get_x_y(n_samples=n_samples)\n",
        "    x=x.reshape((*x.shape, 1))\n",
        "    return train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "x_train, x_test, y_train, y_test = get_data()"
      ],
      "id": "MVI-l8QOnbMF",
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbHPXQq5eUlK",
        "outputId": "ee1baed5-7cdd-46a0-e7b1-533f88a8be19"
      },
      "source": [
        "from keras import regularizers, activations\n",
        "\n",
        "def run():\n",
        "    input_shape = x_train.shape[1:]\n",
        "    vocab_size = input_shape[-2]\n",
        "\n",
        "    config = {}\n",
        "\n",
        "    config[\"dense_layer_5\"] = {\n",
        "        \"units\": 16,\n",
        "        \"activation\": \"relu\",\n",
        "        \"bias_regularizer\": regularizers.l1_l2(0.2, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_5\"] = {\n",
        "        \"units\": 8,\n",
        "        \"activation\": \"relu\",\n",
        "        \"bias_regularizer\": regularizers.l1_l2(0.4, 0.2)\n",
        "    }\n",
        "\n",
        "    # config[\"conv_layer_0\"] = {\n",
        "    #     \"filters\": 1,\n",
        "    #     \"kernel_size\": (2, vocab_size),\n",
        "    #     \"strides\": (1,1),\n",
        "    #     \"activation\": \"relu\",\n",
        "    # }\n",
        "\n",
        "    # config[\"pool_layer_0\"] = {\n",
        "    #     \"pool_size\": (3,3),\n",
        "    #     \"padding\": \"same\"\n",
        "    # }\n",
        "\n",
        "    # config[\"conv_layer_1\"] = {\n",
        "    #     \"filters\": 1,\n",
        "    #     \"kernel_size\": (2, 1),\n",
        "    #     \"strides\": (1,1),\n",
        "    #     \"activation\": \"relu\"\n",
        "    # }\n",
        "\n",
        "    # config[\"pool_layer_1\"] = {\n",
        "    #     \"pool_size\": (2,2),\n",
        "    #     \"padding\": \"same\",\n",
        "    # }\n",
        "\n",
        "    config[\"flatten_layer_0\"] = {\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_0\"] = {\n",
        "        \"units\": 4,\n",
        "        \"activation\": \"relu\",\n",
        "        # \"kernel_regularizer\": regularizers.l1_l2(0.1, 0.2),\n",
        "        # \"bias_regularizer\": regularizers.l1_l2(0.1, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dropout_layer_0\"] = {\n",
        "        \"rate\": 0.5,\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_1\"]= {\n",
        "        \"units\": 8,\n",
        "        \"activation\": \"relu\",\n",
        "        # \"kernel_regularizer\": regularizers.l1_l2(0.1, 0.2),\n",
        "        # \"bias_regularizer\": regularizers.l1_l2(0.1, 0.2)\n",
        "    }\n",
        "\n",
        "    config[\"dropout_layer_1\"]= {\n",
        "        \"rate\": 0.5\n",
        "    }\n",
        "\n",
        "    config[\"dense_layer_final\"] = {\n",
        "        \"units\": (y_train.shape[-1]),\n",
        "        \"activation\": \"softmax\"\n",
        "    }\n",
        "\n",
        "    config[\"comp_model\"]= {\n",
        "        \"optimizer\": tf.keras.optimizers.Adam(0.001),\n",
        "        \"loss\": keras.losses.binary_crossentropy,\n",
        "        \"metrics\": [\"accuracy\", keras.metrics.Precision()]\n",
        "    }\n",
        "\n",
        "    encoder_input = keras.Input(shape=input_shape)\n",
        "    prev_layer = encoder_input\n",
        "    encoder_output=None\n",
        "\n",
        "    layer_types = {\n",
        "        \"dense\": layers.Dense,\n",
        "        \"flatten\": layers.Flatten,\n",
        "        \"dropout\": layers.Dropout,\n",
        "        \"conv\": layers.Conv2D,\n",
        "        \"pool\": layers.MaxPool2D,\n",
        "        \"comp\": None,\n",
        "    }\n",
        "    \n",
        "    for k,v in config.items():\n",
        "      type_of_layer, *_ = k.split(\"_\")\n",
        "      if type_of_layer != \"comp\":\n",
        "        print(v)\n",
        "        encoder_output = layer_types[type_of_layer](**v)(prev_layer)\n",
        "        prev_layer = encoder_output\n",
        "    \n",
        "    model = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "\n",
        "    print(model.summary())\n",
        "    \n",
        "    model.compile(**config[\"comp_model\"])\n",
        "\n",
        "    history = model.fit(x_train, y_train, batch_size=2, epochs=16, validation_split=0.2)\n",
        "\n",
        "    test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "    print(y_train[0])\n",
        "\n",
        "    print(\"Test loss:\", test_scores[0])\n",
        "    print(\"Test accuracy:\", test_scores[1])\n",
        "\n",
        "    return model    \n",
        "    \n",
        "model = run()"
      ],
      "id": "zbHPXQq5eUlK",
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'units': 8, 'activation': 'relu', 'bias_regularizer': <keras.regularizers.L1L2 object at 0x7f9baced9450>}\n",
            "{}\n",
            "{'units': 4, 'activation': 'relu'}\n",
            "{'rate': 0.5}\n",
            "{'units': 8, 'activation': 'relu'}\n",
            "{'rate': 0.5}\n",
            "{'units': 3, 'activation': 'softmax'}\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_68 (InputLayer)       [(None, 56, 4461, 1)]     0         \n",
            "                                                                 \n",
            " dense_156 (Dense)           (None, 56, 4461, 8)       16        \n",
            "                                                                 \n",
            " flatten_53 (Flatten)        (None, 1998528)           0         \n",
            "                                                                 \n",
            " dense_157 (Dense)           (None, 4)                 7994116   \n",
            "                                                                 \n",
            " dropout_90 (Dropout)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_158 (Dense)           (None, 8)                 40        \n",
            "                                                                 \n",
            " dropout_91 (Dropout)        (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_159 (Dense)           (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,994,199\n",
            "Trainable params: 7,994,199\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/16\n",
            "144/144 [==============================] - 11s 69ms/step - loss: 0.7049 - accuracy: 0.3403 - precision_51: 0.2500 - val_loss: 0.6890 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 2/16\n",
            "144/144 [==============================] - 9s 65ms/step - loss: 0.6821 - accuracy: 0.3785 - precision_51: 0.0000e+00 - val_loss: 0.6801 - val_accuracy: 0.4028 - val_precision_51: 0.0000e+00\n",
            "Epoch 3/16\n",
            "144/144 [==============================] - 10s 67ms/step - loss: 0.6759 - accuracy: 0.3611 - precision_51: 0.0000e+00 - val_loss: 0.6714 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 4/16\n",
            "144/144 [==============================] - 10s 68ms/step - loss: 0.6686 - accuracy: 0.3715 - precision_51: 0.0000e+00 - val_loss: 0.6730 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 5/16\n",
            "144/144 [==============================] - 10s 68ms/step - loss: 0.6657 - accuracy: 0.3611 - precision_51: 0.0000e+00 - val_loss: 0.6639 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 6/16\n",
            "144/144 [==============================] - 10s 66ms/step - loss: 0.6558 - accuracy: 0.3750 - precision_51: 0.0000e+00 - val_loss: 0.6557 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 7/16\n",
            "144/144 [==============================] - 10s 68ms/step - loss: 0.6542 - accuracy: 0.3750 - precision_51: 0.5000 - val_loss: 0.6604 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 8/16\n",
            "144/144 [==============================] - 10s 67ms/step - loss: 0.6558 - accuracy: 0.3438 - precision_51: 0.0000e+00 - val_loss: 0.6588 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 9/16\n",
            "144/144 [==============================] - 10s 66ms/step - loss: 0.6602 - accuracy: 0.3889 - precision_51: 0.5000 - val_loss: 0.6602 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 10/16\n",
            "144/144 [==============================] - 10s 68ms/step - loss: 0.6533 - accuracy: 0.3507 - precision_51: 0.0000e+00 - val_loss: 0.6532 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 11/16\n",
            "144/144 [==============================] - 10s 67ms/step - loss: 0.6440 - accuracy: 0.3542 - precision_51: 1.0000 - val_loss: 0.6674 - val_accuracy: 0.2778 - val_precision_51: 0.0000e+00\n",
            "Epoch 12/16\n",
            "144/144 [==============================] - 10s 67ms/step - loss: 0.6599 - accuracy: 0.4028 - precision_51: 0.0000e+00 - val_loss: 0.6661 - val_accuracy: 0.3333 - val_precision_51: 0.0000e+00\n",
            "Epoch 13/16\n",
            "144/144 [==============================] - 10s 69ms/step - loss: 0.6548 - accuracy: 0.3403 - precision_51: 0.0000e+00 - val_loss: 0.6562 - val_accuracy: 0.2778 - val_precision_51: 0.0000e+00\n",
            "Epoch 14/16\n",
            "144/144 [==============================] - 10s 67ms/step - loss: 0.6462 - accuracy: 0.3542 - precision_51: 0.0000e+00 - val_loss: 0.6553 - val_accuracy: 0.2778 - val_precision_51: 0.0000e+00\n",
            "Epoch 15/16\n",
            "144/144 [==============================] - 10s 67ms/step - loss: 0.6470 - accuracy: 0.3889 - precision_51: 0.0000e+00 - val_loss: 0.6537 - val_accuracy: 0.2778 - val_precision_51: 0.0000e+00\n",
            "Epoch 16/16\n",
            "144/144 [==============================] - 10s 67ms/step - loss: 0.6428 - accuracy: 0.3403 - precision_51: 0.0000e+00 - val_loss: 0.6474 - val_accuracy: 0.2778 - val_precision_51: 0.0000e+00\n",
            "8/8 - 2s - loss: 0.6459 - accuracy: 0.3042 - precision_51: 0.0000e+00 - 2s/epoch - 309ms/step\n",
            "[0 0 1]\n",
            "Test loss: 0.6459187865257263\n",
            "Test accuracy: 0.30416667461395264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y_93rstMHJ_s",
        "outputId": "dfc6e721-79f9-4dfc-cf5f-856833c3fad0"
      },
      "source": [
        "result = unpack_dict_list(get_cleaned_data_from_folder(data_target_path))\n",
        "\n",
        "for i in result:\n",
        "  print(i)\n",
        "\n",
        "  word = clean_up_user_input(i)\n",
        "  word = word.reshape(*word.shape, 1)\n",
        "  words = np.array([word])\n",
        "  output_encoder: LabelBinarizer = pickle.load(open(model_path + output_encoder_file_name, \"rb\"))\n",
        "  prediction = model.predict(words)\n",
        "  print(prediction)\n",
        "  prediction_binary = np.zeros_like(prediction)\n",
        "  prediction_binary[:,prediction.argmax(1)] = 1\n",
        "  print(prediction_binary)\n",
        "  print(prediction)\n",
        "  print(output_encoder.inverse_transform(prediction))\n",
        "\n",
        "\n"
      ],
      "id": "y_93rstMHJ_s",
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "introduct\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "migrain\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "common\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "episod\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "disord\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "hallmark\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "disabl\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "headach\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "gener\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "associ\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "nausea\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "light\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "sound\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "sensit\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "[[1. 0. 0.]]\n",
            "[[0.36861306 0.35012847 0.28125846]]\n",
            "['depression']\n",
            "the\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-355-7190104244b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_up_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-338-db5cd66d9fa1>\u001b[0m in \u001b[0;36mclean_up_user_input\u001b[0;34m(sentence, word_limit)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0minput_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_encoder_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mencoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_data_by_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mencoded_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mzero_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-336-e68b6d3de7d6>\u001b[0m in \u001b[0;36mtransform_data_by_encoder\u001b[0;34m(data, encoder)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_data_by_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mencoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             \u001b[0mwarn_on_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_unknown\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m         )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         X_list, n_samples, n_features = self._check_X(\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m             )\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
          ]
        }
      ]
    }
  ]
}